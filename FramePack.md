```bash

sudo apt-get update && sudo apt-get install git-lfs ffmpeg cbm 

git clone -b frame-pack-inference https://github.com/kohya-ss/musubi-tuner.git
cd musubi-tuner

pip install torch torchvision
pip install -r requirements.txt
pip install ascii-magic matplotlib tensorboard huggingface_hub datasets
pip install moviepy==1.0.3
pip install sageattention==1.0.6

git clone https://huggingface.co/lllyasviel/FramePackI2V_HY
->
--dit FramePackI2V_HY/diffusion_pytorch_model-00001-of-00003.safetensors

huggingface-cli download tencent/HunyuanVideo --local-dir ./ckpts
->
ckpts/hunyuan-video-t2v-720p/vae/pytorch_model.pt
OR 
git clone https://huggingface.co/hunyuanvideo-community/HunyuanVideo
->
--vae HunyuanVideo/vae/diffusion_pytorch_model.safetensors

git clone https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged
->
--text_encoder1 HunyuanVideo_repackaged/split_files/text_encoders/llava_llama3_fp16.safetensors
--text_encoder2 HunyuanVideo_repackaged/split_files/text_encoders/clip_l.safetensors 

git clone https://huggingface.co/Comfy-Org/sigclip_vision_384
->
--image_encoder sigclip_vision_384/sigclip_vision_patch14_384.safetensors


```

#### ç¤ºä¾‹è®­ç»ƒ
```python
#### bigger than 37
import os
import shutil
from moviepy.editor import VideoFileClip

# å®šä¹‰è·¯å¾„
src_dir = "Yi_Chen_Dancing_Animation_Videos_White_Background_Splited_Captioned_960x544x6"
dst_dir = "Yi_Chen_Dancing_Animation_Videos_White_Background_Splited_Captioned_960x544x6_upper_60fm"

# åˆ›å»ºç›®æ ‡ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
os.makedirs(dst_dir, exist_ok=True)

# éå†æºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
for filename in os.listdir(src_dir):
    if filename.endswith(".mp4"):
        mp4_path = os.path.join(src_dir, filename)
        txt_path = os.path.join(src_dir, filename.replace(".mp4", ".txt"))

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¯¹åº”çš„.txtæ–‡ä»¶
        if not os.path.exists(txt_path):
            print(f"è­¦å‘Š: æœªæ‰¾åˆ° {filename} å¯¹åº”çš„ .txt æ–‡ä»¶ï¼Œè·³è¿‡ã€‚")
            continue

        # ä½¿ç”¨MoviePyè·å–è§†é¢‘å¸§æ•°
        try:
            with VideoFileClip(mp4_path) as video:
                frame_count = int(video.duration * video.fps)
                print(f"å¤„ç†: {filename} | å¸§æ•°: {frame_count}")

                # å¦‚æœå¸§æ•°>60ï¼Œæ‹·è´æ–‡ä»¶å¯¹
                if frame_count > 60:
                    shutil.copy2(mp4_path, dst_dir)
                    shutil.copy2(txt_path, dst_dir)
                    print(f"å·²æ‹·è´: {filename} å’ŒåŒå .txt æ–‡ä»¶åˆ° {dst_dir}")
        except Exception as e:
            print(f"é”™è¯¯: å¤„ç† {filename} æ—¶å‡ºé”™ - {str(e)}")

print("å¤„ç†å®Œæˆï¼")
```

```toml
[general]
resolution = [960, 544]
caption_extension = ".txt"
batch_size = 1
enable_bucket = true
bucket_no_upscale = false

[[datasets]]
video_directory = "Yi_Chen_Dancing_Animation_Videos_White_Background_Splited_Captioned_960x544x6_upper_60fm"
cache_directory = "Yi_Chen_Dancing_Animation_Videos_White_Background_Splited_Captioned_960x544x6_upper_60fm_cache"
target_frames = [1, 25, 79]
max_frames = 129
frame_extraction = "full"
```

```toml
[general]
resolution = [960, 544]
caption_extension = ".txt"
batch_size = 1
enable_bucket = true
bucket_no_upscale = false

[[datasets]]
video_directory = "Yi_Chen_Dancing_Animation_Videos_White_Background_Splited_Captioned_960x544x6_upper_60fm"
cache_directory = "Yi_Chen_Dancing_Animation_Videos_White_Background_Splited_Captioned_960x544x6_upper_60fm_45_cache"
target_frames = [1, 25, 45]
max_frames = 45
frame_extraction = "full"
```

```bash
python fpack_cache_latents.py \
    --dataset_config video_config.toml \
    --vae HunyuanVideo/vae/diffusion_pytorch_model.safetensors \
    --image_encoder sigclip_vision_384/sigclip_vision_patch14_384.safetensors \
    --vae_chunk_size 32 --vae_spatial_tile_sample_min_size 128

python fpack_cache_text_encoder_outputs.py \
    --dataset_config video_config.toml \
    --text_encoder1 HunyuanVideo_repackaged/split_files/text_encoders/llava_llama3_fp16.safetensors \
    --text_encoder2 HunyuanVideo_repackaged/split_files/text_encoders/clip_l.safetensors \
    --batch_size 16

accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 fpack_train_network.py \
    --dit FramePackI2V_HY/diffusion_pytorch_model-00001-of-00003.safetensors \
    --vae HunyuanVideo/vae/diffusion_pytorch_model.safetensors \
    --text_encoder1 HunyuanVideo_repackaged/split_files/text_encoders/llava_llama3_fp16.safetensors \
    --text_encoder2 HunyuanVideo_repackaged/split_files/text_encoders/clip_l.safetensors \
    --image_encoder sigclip_vision_384/sigclip_vision_patch14_384.safetensors \
    --dataset_config video_config.toml \
    --sdpa --mixed_precision bf16 \
    --optimizer_type adamw8bit --learning_rate 2e-4 --gradient_checkpointing \
    --timestep_sampling shift --weighting_scheme none --discrete_flow_shift 3.0 \
    --max_data_loader_n_workers 2 --persistent_data_loader_workers \
    --network_module networks.lora_framepack --network_dim 32 \
    --max_train_epochs 500 --save_every_n_epochs 1 --seed 42 \
    --output_dir framepack_yichen_output --output_name framepack-yichen-lora
```

```bash
python fpack_generate_video.py \
    --dit FramePackI2V_HY/diffusion_pytorch_model-00001-of-00003.safetensors \
    --vae HunyuanVideo/vae/diffusion_pytorch_model.safetensors \
    --text_encoder1 HunyuanVideo_repackaged/split_files/text_encoders/llava_llama3_fp16.safetensors \
    --text_encoder2 HunyuanVideo_repackaged/split_files/text_encoders/clip_l.safetensors \
    --image_encoder sigclip_vision_384/sigclip_vision_patch14_384.safetensors \
    --image_path fln.png \
    --prompt "In the style of Yi Chen Dancing White Background , The character's movements shift dynamically throughout the video, transitioning from poised stillness to lively dance steps. Her expressions evolve seamlesslyâ€”starting with focused determination, then flashing surprise as she executes a quick spin, before breaking into a joyful smile mid-leap. Her hands flow through choreographed positions, sometimes extending gracefully like unfolding wings, other times clapping rhythmically against her wrists. During a dramatic hip sway, her fingers fan open near her cheek, then sweep downward as her whole body dips into a playful crouch, the sequins on her costume catching the light with every motion." \
    --video_size 960 544 --video_seconds 3 --fps 30 --infer_steps 25 \
    --attn_mode sdpa --fp8_scaled \
    --vae_chunk_size 32 --vae_spatial_tile_sample_min_size 128 \
    --save_path save --output_type both \
    --seed 1234 --lora_multiplier 1.0 --lora_weight framepack_yichen_output/framepack-yichen-lora-000002.safetensors
```




https://github.com/user-attachments/assets/e695d2a7-4145-4cf2-b9b5-2f4ca764ec02


#### å°¼éœ²ä¾‹å­
<br/>

```bash
wget https://huggingface.co/tori29umai/FramePack_LoRA/resolve/main/rotate_landscape_V4.safetensors

huggingface-cli download --repo-type dataset \
    --resume-download \
    svjack/Genshin-Impact-Portrait-with-Tags-Filtered-IID-Gender-SP \
    --include "genshin_impact_NILOU_images_and_texts/*" \
    --local-dir .
```

```python
import os
import glob
import shutil
from tqdm import tqdm
import subprocess
import time

def get_latest_mp4(save_dir):
    """è·å–saveç›®å½•ä¸‹æœ€æ–°åˆ›å»ºçš„mp4æ–‡ä»¶"""
    mp4_files = glob.glob(os.path.join(save_dir, '*.mp4'))
    if not mp4_files:
        return None
    return max(mp4_files, key=os.path.getctime)

def process_images_and_texts():
    # è¾“å…¥å’Œè¾“å‡ºç›®å½•
    input_dir = "genshin_impact_NILOU_images_and_texts"
    output_dir = "genshin_impact_NILOU_FramePack_Rotate_Dancing_Captioned"
    save_dir = "save"

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(save_dir, exist_ok=True)

    # è·å–æ‰€æœ‰.pngæ–‡ä»¶
    png_files = glob.glob(os.path.join(input_dir, '*.png'))

    # å›ºå®šprompt
    prompt = """In the style of Yi Chen Dancing White Background , The character's movements shift dynamically throughout the video, transitioning from poised stillness to lively dance steps. Her expressions evolve seamlesslyâ€”starting with focused determination, then flashing surprise as she executes a quick spin, before breaking into a joyful smile mid-leap. Her hands flow through choreographed positions, sometimes extending gracefully like unfolding wings, other times clapping rhythmically against her wrists. During a dramatic hip sway, her fingers fan open near her cheek, then sweep downward as her whole body dips into a playful crouch, the sequins on her costume catching the light with every motion."""

    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
    for png_file in tqdm(png_files, desc="Processing images"):
        # è·å–å¯¹åº”çš„txtæ–‡ä»¶
        base_name = os.path.splitext(os.path.basename(png_file))[0]
        txt_file = os.path.join(input_dir, f"{base_name}.txt")

        if not os.path.exists(txt_file):
            print(f"Warning: No corresponding .txt file found for {png_file}")
            continue

        # æ„å»ºå‘½ä»¤
        cmd = [
            "python", "fpack_generate_video.py",
            "--dit", "FramePackI2V_HY/diffusion_pytorch_model-00001-of-00003.safetensors",
            "--vae", "HunyuanVideo/vae/diffusion_pytorch_model.safetensors",
            "--text_encoder1", "HunyuanVideo_repackaged/split_files/text_encoders/llava_llama3_fp16.safetensors",
            "--text_encoder2", "HunyuanVideo_repackaged/split_files/text_encoders/clip_l.safetensors",
            "--image_encoder", "sigclip_vision_384/sigclip_vision_patch14_384.safetensors",
            "--image_path", png_file,
            "--prompt", prompt,
            "--video_size", "960", "544",
            "--video_seconds", "3",
            "--fps", "30",
            "--infer_steps", "25",
            "--attn_mode", "sdpa",
            "--fp8_scaled",
            "--vae_chunk_size", "32",
            "--vae_spatial_tile_sample_min_size", "128",
            "--save_path", save_dir,
            "--output_type", "both",
            "--seed", "1234",
            "--lora_multiplier", "1.0",
            "--lora_weight", "rotate_landscape_V4.safetensors"
        ]

        # è¿è¡Œå‘½ä»¤
        try:
            subprocess.run(cmd, check=True)
        except subprocess.CalledProcessError as e:
            print(f"Error processing {png_file}: {e}")
            continue

        # è·å–æœ€æ–°ç”Ÿæˆçš„mp4æ–‡ä»¶
        latest_mp4 = get_latest_mp4(save_dir)
        if latest_mp4 is None:
            print(f"Warning: No .mp4 file generated for {png_file}")
            continue

        # æ„å»ºè¾“å‡ºæ–‡ä»¶å
        output_mp4 = os.path.join(output_dir, f"{base_name}.mp4")
        output_txt = os.path.join(output_dir, f"{base_name}.txt")

        # æ‹·è´æ–‡ä»¶
        shutil.move(latest_mp4, output_mp4)
        shutil.copy2(txt_file, output_txt)

        '''
        # æ¸…ç†saveç›®å½•
        for f in glob.glob(os.path.join(save_dir, '*')):
            try:
                os.remove(f)
            except:
                pass
        '''

if __name__ == "__main__":
    process_images_and_texts()
    print("All processing completed!")
```

#### åŸç¥ç”Ÿè´ºä¾‹å­

```python
#### git clone https://huggingface.co/datasets/svjack/Genshin_Impact_Birthday_Art_Images

import os
import glob
import shutil
from tqdm import tqdm
import subprocess
import time

def get_latest_mp4(save_dir):
    """è·å–saveç›®å½•ä¸‹æœ€æ–°åˆ›å»ºçš„mp4æ–‡ä»¶"""
    mp4_files = glob.glob(os.path.join(save_dir, '*.mp4'))
    if not mp4_files:
        return None
    return max(mp4_files, key=os.path.getctime)

def process_images_and_texts():
    # è¾“å…¥å’Œè¾“å‡ºç›®å½•
    input_dir = "Genshin_Impact_Birthday_Art_Images"
    output_dir = "Genshin_Impact_Birthday_Art_FramePack_Rotate_Dancing_Captioned_New"
    save_dir = "save"

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(save_dir, exist_ok=True)

    # è·å–æ‰€æœ‰.pngæ–‡ä»¶
    png_files_0 = glob.glob(os.path.join(input_dir, '*.png'))
    png_files_1 = glob.glob(os.path.join(input_dir, '*.jpg'))
    png_files_2 = glob.glob(os.path.join(input_dir, '*.jpeg'))
    png_files_3 = glob.glob(os.path.join(input_dir, '*.webp'))
    png_files = list(png_files_0) + list(png_files_1) + list(png_files_2) + list(png_files_3)


    # å›ºå®šprompt
    prompt = """The camera smoothly orbits around the center of the scene, keeping the center point fixed and always in view."""

    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
    for png_file in tqdm(png_files, desc="Processing images"):
        # è·å–å¯¹åº”çš„txtæ–‡ä»¶
        base_name = os.path.splitext(os.path.basename(png_file))[0]
        txt_file = os.path.join(input_dir, f"{base_name}.txt")

        '''
        if not os.path.exists(txt_file):
            print(f"Warning: No corresponding .txt file found for {png_file}")
            continue
        '''
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            "python", "fpack_generate_video.py",
            "--dit", "FramePackI2V_HY/diffusion_pytorch_model-00001-of-00003.safetensors",
            "--vae", "HunyuanVideo/vae/diffusion_pytorch_model.safetensors",
            "--text_encoder1", "HunyuanVideo_repackaged/split_files/text_encoders/llava_llama3_fp16.safetensors",
            "--text_encoder2", "HunyuanVideo_repackaged/split_files/text_encoders/clip_l.safetensors",
            "--image_encoder", "sigclip_vision_384/sigclip_vision_patch14_384.safetensors",
            "--image_path", png_file,
            "--prompt", prompt,
            "--video_size", "768", "768",
            "--video_seconds", "3",
            "--fps", "30",
            "--infer_steps", "25",
            "--attn_mode", "sdpa",
            "--fp8_scaled",
            "--vae_chunk_size", "32",
            "--vae_spatial_tile_sample_min_size", "128",
            "--save_path", save_dir,
            "--output_type", "both",
            "--seed", "1234",
            "--lora_multiplier", "1.0",
            "--lora_weight", "rotate_landscape_V4.safetensors"
        ]

        # è¿è¡Œå‘½ä»¤
        try:
            subprocess.run(cmd, check=True)
        except subprocess.CalledProcessError as e:
            print(f"Error processing {png_file}: {e}")
            continue

        # è·å–æœ€æ–°ç”Ÿæˆçš„mp4æ–‡ä»¶
        latest_mp4 = get_latest_mp4(save_dir)
        if latest_mp4 is None:
            print(f"Warning: No .mp4 file generated for {png_file}")
            continue

        # æ„å»ºè¾“å‡ºæ–‡ä»¶å
        output_mp4 = os.path.join(output_dir, f"{base_name}.mp4")
        output_txt = os.path.join(output_dir, f"{base_name}.txt")

        # æ‹·è´æ–‡ä»¶
        shutil.move(latest_mp4, output_mp4)
        #shutil.copy2(txt_file, output_txt)

        '''
        # æ¸…ç†saveç›®å½•
        for f in glob.glob(os.path.join(save_dir, '*')):
            try:
                os.remove(f)
            except:
                pass
        '''

if __name__ == "__main__":
    process_images_and_texts()
    print("All processing completed!")


```

#### åŸç¥ç”Ÿè´º æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½  äºŒåˆ›

##### éŸ³é¢‘srtåˆ†å‰²
```python
# git clone https://huggingface.co/datasets/svjack/I_prefer_you_over_something_TWO_VIDEOS


import os
import re
from pydub import AudioSegment
from moviepy.editor import VideoFileClip
from datetime import datetime, timedelta

def parse_srt(srt_content):
    """Parse SRT content into a list of subtitle segments with duration validation"""
    segments = []
    pattern = re.compile(r'(\d+)\n(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})\n(.+?)(?:\n\n|\n$)', re.DOTALL)

    for match in pattern.finditer(srt_content):
        index = int(match.group(1))
        start_time = parse_srt_time(match.group(2))
        end_time = parse_srt_time(match.group(3))
        text = match.group(4).strip()

        # Validate time range
        if end_time <= start_time:
            print(f"Warning: Skipping segment {index} - End time {end_time} <= Start time {start_time}")
            continue

        segments.append((index, start_time, end_time, text))

    return segments

def parse_srt_time(time_str):
    """Convert SRT time format to seconds with validation"""
    try:
        hours, mins, secs = time_str.split(':')
        secs, millis = secs.split(',')
        return timedelta(
            hours=int(hours),
            minutes=int(mins),
            seconds=int(secs),
            milliseconds=int(millis)
        ).total_seconds()
    except Exception as e:
        raise ValueError(f"Invalid time format '{time_str}': {str(e)}")

def sanitize_filename(filename):
    """Sanitize filenames with special characters"""
    # Keep Chinese/Japanese characters and basic symbols
    return re.sub(r'[<>:"/\\|?*\x00-\x1f]', '', filename).strip()

def get_video_duration(video_path):
    """Get accurate video duration in seconds"""
    try:
        with VideoFileClip(video_path) as video:
            return video.duration
    except Exception as e:
        print(f"Warning: Could not get duration for {video_path}: {str(e)}")
        return None

def adjust_segment_times(segments, max_duration):
    """Adjust segment times to fit within video duration"""
    adjusted_segments = []
    for idx, start, end, text in segments:
        # Cap end time at video duration
        if max_duration and end > max_duration:
            print(f"Adjusting segment {idx} end time from {end}s to {max_duration}s")
            end = max_duration

        # Skip if start time is beyond video duration
        if max_duration and start >= max_duration:
            print(f"Skipping segment {idx} - Start time {start}s >= video duration {max_duration}s")
            continue

        # Skip if duration becomes zero after adjustment
        if end <= start:
            print(f"Skipping segment {idx} - Invalid duration after adjustment (start: {start}s, end: {end}s)")
            continue

        adjusted_segments.append((idx, start, end, text))
    return adjusted_segments

def split_media(input_path, srt_content, output_path, output_type='wav'):
    """Improved media splitting with duration validation"""
    os.makedirs(output_path, exist_ok=True)

    try:
        # Get video duration first
        video_duration = None
        if output_type == 'mp4':
            video_duration = get_video_duration(input_path)

        # Parse and validate segments
        segments = parse_srt(srt_content)
        if video_duration:
            segments = adjust_segment_times(segments, video_duration)

        if not segments:
            print("No valid segments to process")
            return False

        input_name = os.path.splitext(os.path.basename(input_path))[0]
        clean_name = sanitize_filename(input_name)

        if output_type == 'wav':
            audio = AudioSegment.from_file(input_path)
            audio_duration = len(audio) / 1000  # pydub uses milliseconds

            for idx, start_time, end_time, text in segments:
                # Convert to milliseconds and validate against audio duration
                start_ms = int(start_time * 1000)
                end_ms = int(end_time * 1000)

                if end_ms > len(audio):
                    end_ms = len(audio)
                    print(f"Adjusting audio segment {idx} end time to {end_ms/1000}s")

                segment = audio[start_ms:end_ms]

                base_name = f"{idx:04d}_{clean_name}"
                audio_file = os.path.join(output_path, f"{base_name}.wav")
                text_file = os.path.join(output_path, f"{base_name}.txt")

                segment.export(audio_file, format='wav')
                with open(text_file, 'w', encoding='utf-8') as f:
                    f.write(text)

        elif output_type == 'mp4':
            with VideoFileClip(input_path) as video:
                for idx, start_time, end_time, text in segments:
                    # Ensure times are within video duration
                    if end_time > video.duration:
                        end_time = video.duration
                        print(f"Adjusting video segment {idx} end time to {end_time}s")

                    base_name = f"{idx:04d}_{clean_name}"
                    video_file = os.path.join(output_path, f"{base_name}.mp4")
                    text_file = os.path.join(output_path, f"{base_name}.txt")

                    segment = video.subclip(start_time, end_time)
                    segment.write_videofile(
                        video_file,
                        codec='libx264',
                        audio_codec='aac',
                        verbose=False,
                        threads=4,
                        preset='fast',
                        ffmpeg_params=['-movflags', '+faststart']
                    )
                    with open(text_file, 'w', encoding='utf-8') as f:
                        f.write(text)

        return True

    except Exception as e:
        print(f"Error processing {input_path}: {str(e)}")
        return False

# Main execution with enhanced error handling
if __name__ == "__main__":
    video_files = [
        "trimmed_ğŸ™‹æ¯”èµ·ğŸ¦æˆ‘æ›´å–œæ¬¢å›½ç”·v.mp4",
        "trimmed_[åŸç¥åŠ¨ç”»]æˆ‘æ›´å–œæ¬¢ä½ ï¼æ¯”å¿ƒ.mp4"
    ]

    # SRT contents
    srt_contents = {
    "ja": """
1
00:00:00,000 --> 00:00:02,050
ã©ã†ã

2
00:00:02,050 --> 00:00:04,050
ä½•ãŒå¥½ãï¼Ÿ

3
00:00:04,050 --> 00:00:07,100
ãƒŸãƒ³ãƒˆã‚ˆã‚Šã‚‚ã‚ãªãŸãŒå¥½ã

4
00:00:07,100 --> 00:00:11,100
ã‚«ã‚¿ãƒˆã¡ã‚ƒã‚“ã€ä½•ãŒå¥½ãï¼Ÿ

5
00:00:11,100 --> 00:00:14,300
ã‚¹ãƒˆãƒ­ãƒ™ãƒªãƒ¼ã‚ˆã‚Šã‚‚ã‚ãªãŸãŒå¥½ã

6
00:00:14,300 --> 00:00:18,150
èµ¤ã¡ã‚ƒã‚“ã€ä½•ãŒå¥½ãï¼Ÿ

7
00:00:18,150 --> 00:00:23,000
ã‚¯ãƒƒã‚­ãƒ¼ã‚¯ãƒªãƒ¼ãƒ ã‚ˆã‚Šã‚‚ã‚ãªãŸãŒå¥½ã
""",
    "zh": """
1
00:00:00,000 --> 00:00:02,050
è¯·è¯´å§

2
00:00:02,050 --> 00:00:04,050
ä½ å–œæ¬¢ä»€ä¹ˆï¼Ÿ

3
00:00:04,050 --> 00:00:07,100
æ¯”èµ·è–„è·å‘³æˆ‘æ›´å–œæ¬¢ä½ 

4
00:00:07,100 --> 00:00:11,100
å¡å¡”æ‰˜é…±ï¼Œä½ å–œæ¬¢ä»€ä¹ˆï¼Ÿ

5
00:00:11,100 --> 00:00:14,300
æ¯”èµ·è‰è“å‘³æˆ‘æ›´å–œæ¬¢ä½ 

6
00:00:14,300 --> 00:00:18,150
å®å®å–œæ¬¢ä»€ä¹ˆï¼Ÿ

7
00:00:18,150 --> 00:00:23,000
æ¯”èµ·é¥¼å¹²å¥¶æ²¹å‘³æˆ‘æ›´å–œæ¬¢ä½ 
""",
    "en": """
1
00:00:00,000 --> 00:00:02,050
Please tell me

2
00:00:02,050 --> 00:00:04,050
What do you like?

3
00:00:04,050 --> 00:00:07,100
I love you more than mint

4
00:00:07,100 --> 00:00:11,100
Katato-chan, what do you like?

5
00:00:11,100 --> 00:00:14,300
I love you more than strawberry

6
00:00:14,300 --> 00:00:18,150
Baby, what do you like?

7
00:00:18,150 --> 00:00:23,000
I love you more than cookies and cream
"""
}
    #output_types = ['mp4', 'wav']
    output_types = ['wav']

    for video in video_files:
        if not os.path.exists(video):
            print(f"Error: Input file not found - {video}")
            continue

        for lang, srt in srt_contents.items():
            for out_type in output_types:
                base_name = os.path.splitext(video)[0]
                output_dir = f"output_{lang}_{out_type}_{sanitize_filename(base_name)}"

                print(f"\nProcessing: {video} | {lang} | {out_type}")
                print(f"Output to: {output_dir}")

                success = split_media(
                    input_path=video,
                    srt_content=srt,
                    output_path=output_dir,
                    output_type=out_type
                )

                if success:
                    print("âœ“ Successfully processed")
                else:
                    print("âœ— Processing failed")

```

##### åˆçº§éŸ³è§†é¢‘æ‹¼æ¥
```python
#git clone https://huggingface.co/datasets/svjack/I_prefer_you_over_something_GIRL_AUDIOS_SPLITED
#git clone https://huggingface.co/datasets/svjack/I_prefer_you_over_something_BOY_AUDIOS_SPLITED
#git clone https://huggingface.co/datasets/svjack/Genshin-Impact-Portrait-with-Tags-Filtered-IID-Gender-SP
#git clone https://huggingface.co/datasets/svjack/Genshin_Impact_Birthday_Art_FramePack_Rotate_Named


import os
import random
import pandas as pd
from itertools import combinations
import subprocess
from moviepy.editor import concatenate_videoclips, AudioFileClip, ImageClip, CompositeAudioClip, VideoFileClip
from moviepy.video.fx import all as vfx
from moviepy.video.fx.all import crop
from moviepy.audio.AudioClip import AudioClip
from PIL import Image
import numpy as np

# é…ç½®å‚æ•°
config = {
    'num_images_per_char': 3,  # æ¯ä¸ªè§’è‰²ä½¿ç”¨çš„å›¾ç‰‡æ•°é‡
    'output_resolution': (1024, 1024),  # è¾“å‡ºè§†é¢‘åˆ†è¾¨ç‡
    'image_duration': 3,  # æ¯å¼ å›¾ç‰‡æ˜¾ç¤ºæ—¶é•¿(ç§’)
    'video_duration': 5,  # æ¯ä¸ªè§†é¢‘ç‰‡æ®µæ—¶é•¿(ç§’)
    'audio_fade_duration': 0.5,  # éŸ³é¢‘æ·¡å…¥æ·¡å‡ºæ—¶é•¿(ç§’)
}

# è§’è‰²åå­—æ˜ å°„å’Œæ€§åˆ«æ˜ å°„
name_mapping = {
    'èŠ­èŠ­æ‹‰': 'BARBARA', 'æŸ¯è±': 'COLLEI', 'é›·ç”µå°†å†›': 'RAIDEN SHOGUN', 'äº‘å ‡': 'YUN JIN',
    'å…«é‡ç¥å­': 'YAE MIKO', 'å¦®éœ²': 'NILOU', 'ç»®è‰¯è‰¯': 'KIRARA', 'ç ‚ç³–': 'SUCROSE',
    'çéœ²çŠ': 'FARUZAN', 'ç³å¦®ç‰¹': 'LYNETTE', 'çº³è¥¿å¦²': 'NAHIDA', 'è¯ºè‰¾å°”': 'NOELLE',
    'å‡å…‰': 'NINGGUANG', 'é¹¿é‡é™¢å¹³è—': 'HEIZOU', 'ç´': 'JEAN', 'æ«åŸä¸‡å¶': 'KAEDEHARA KAZUHA',
    'èŠ™å®å¨œ': 'FURINA', 'è‰¾å°”æµ·æ£®': 'ALHAITHAM', 'ç”˜é›¨': 'GANYU', 'å‡¯äºš': 'KAEYA',
    'è’æ³·ä¸€æ–—': 'ARATAKI ITTO', 'ä¼˜èˆ': 'EULA', 'è¿ªå¥¥å¨œ': 'DIONA', 'æ¸©è¿ª': 'VENTI',
    'ç¥é‡Œç»«äºº': 'KAMISATO AYATO', 'é˜¿è´å¤š': 'ALBEDO', 'é‡äº‘': 'CHONGYUN', 'é’Ÿç¦»': 'ZHONGLI',
    'è¡Œç§‹': 'XINGQIU', 'èƒ¡æ¡ƒ': 'HU TAO', 'é­ˆ': 'XIAO', 'èµ›è¯º': 'CYNO',
    'ç¥é‡Œç»«å': 'KAMISATO AYAKA', 'äº”éƒ': 'GOROU', 'æ—å°¼': 'LYNEY', 'è¿ªå¢å…‹': 'DILUC',
    'å®‰æŸ': 'AMBER', 'çƒŸç»¯': 'YANFEI', 'å®µå®«': 'YOIMIYA', 'çŠç‘šå®«å¿ƒæµ·': 'SANGONOMIYA KOKOMI',
    'ç½—èè‰äºš': 'ROSARIA', 'ä¸ƒä¸ƒ': 'QIQI', 'ä¹…å²å¿': 'KUKI SHINOBU', 'ç”³é¹¤': 'SHENHE',
    'æ‰˜é©¬': 'THOMA', 'èŠ™å¯§å¨œ': 'FURINA', 'é›·æ³½': 'RAZOR'
}

gender_mapping = {
    'ä¹…å²å¿': 'å¥³', 'äº‘å ‡': 'å¥³', 'äº”éƒ': 'ç”·', 'ä¼˜èˆ': 'å¥³', 'å‡å…‰': 'å¥³', 'å‡¯äºš': 'ç”·',
    'å®‰æŸ': 'å¥³', 'å®µå®«': 'å¥³', 'æ¸©è¿ª': 'ç”·', 'çƒŸç»¯': 'å¥³', 'çŠç‘šå®«å¿ƒæµ·': 'å¥³', 'ç´': 'å¥³',
    'ç”˜é›¨': 'å¥³', 'ç”³é¹¤': 'å¥³', 'ç ‚ç³–': 'å¥³', 'ç¥é‡Œç»«äºº': 'ç”·', 'ç¥é‡Œç»«å': 'å¥³', 'ç»®è‰¯è‰¯': 'å¥³',
    'ç½—èè‰äºš': 'å¥³', 'èƒ¡æ¡ƒ': 'å¥³', 'è‰¾å°”æµ·æ£®': 'ç”·', 'è’æ³·ä¸€æ–—': 'ç”·', 'è¡Œç§‹': 'ç”·', 'è¯ºè‰¾å°”': 'å¥³',
    'è¿ªå¢å…‹': 'ç”·', 'è¿ªå¥¥å¨œ': 'å¥³', 'é‡äº‘': 'ç”·', 'é’Ÿç¦»': 'ç”·', 'é˜¿è´å¤š': 'ç”·', 'é›·æ³½': 'ç”·',
    'é›·ç”µå°†å†›': 'å¥³', 'é­ˆ': 'ç”·', 'é¹¿é‡é™¢å¹³è—': 'ç”·'
}

# è·¯å¾„é…ç½®
paths = {
    'female_audios': 'I_prefer_you_over_something_GIRL_AUDIOS_SPLITED',
    'male_audios': 'I_prefer_you_over_something_BOY_AUDIOS_SPLITED',
    'images': 'Genshin-Impact-Portrait-with-Tags-Filtered-IID-Gender-SP',
    'videos': 'Genshin_Impact_Birthday_Art_FramePack_Rotate_Named'
}

# åŠ è½½è§†é¢‘å…ƒæ•°æ®
video_metadata = pd.read_csv(os.path.join(paths['videos'], 'metadata.csv'))

def get_character_resources(gender, num_groups):
    """è·å–æŒ‡å®šæ€§åˆ«çš„è§’è‰²ç»„åˆå’Œèµ„æº"""
    # æŒ‰æ€§åˆ«ç­›é€‰è§’è‰²
    chars = [name for name, g in gender_mapping.items() if g == gender]

    # ç”Ÿæˆæ‰€æœ‰3è§’è‰²ç»„åˆ
    all_combinations = list(combinations(chars, 3))
    random.shuffle(all_combinations)

    # åªå–éœ€è¦çš„æ•°é‡
    selected_combinations = all_combinations[:num_groups]

    results = []
    for combo in selected_combinations:
        group_data = []
        for char in combo:
            # è·å–è§’è‰²è‹±æ–‡å
            en_name = name_mapping.get(char, char).replace(' ', '_').upper()

            # è·å–éŸ³é¢‘æ–‡ä»¶
            audio_dir = paths['female_audios'] if gender == 'å¥³' else paths['male_audios']
            audio_files = sorted([f for f in os.listdir(audio_dir) if f.endswith('.wav') or f.endswith('.mp3')])

            # è·å–å›¾ç‰‡æ–‡ä»¶
            image_dir = os.path.join(paths['images'], f'genshin_impact_{en_name}_images_and_texts')
            image_files = []
            if os.path.exists(image_dir):
                image_files = [f for f in os.listdir(image_dir) if f.lower().endswith('.png')]
                random.shuffle(image_files)
                image_files = image_files[:config['num_images_per_char']]

            # è·å–è§†é¢‘æ–‡ä»¶
            video_files = video_metadata[video_metadata['prompt'] == char]['file_name'].tolist()

            group_data.append({
                'name': char,
                'en_name': en_name,
                'audio_files': audio_files,
                'image_files': [os.path.join(image_dir, f) for f in image_files],
                'video_files': [os.path.join(paths['videos'], f) for f in video_files]
            })

        results.append(group_data)

    return results

def create_video_clip(group_data, output_path):
    """ä¸ºå•ä¸ªè§’è‰²ç»„åˆåˆ›å»ºè§†é¢‘ï¼ˆæ ¹æ®éŸ³é¢‘æ—¶é—´åˆ†é…å›¾ç‰‡æ˜¾ç¤ºæ—¶é—´ï¼‰"""
    clips = []
    audio_clips = []
    current_time = 0  # è·Ÿè¸ªå½“å‰æ—¶é—´ä½ç½®

    # è·å–éŸ³é¢‘æ–‡ä»¶
    audio_dir = paths['female_audios'] if gender_mapping[group_data[0]['name']] == 'å¥³' else paths['male_audios']
    audio_files = sorted([f for f in os.listdir(audio_dir) if f.endswith(('.wav', '.mp3'))])

    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„éŸ³é¢‘æ–‡ä»¶ï¼ˆ7ä¸ªï¼‰
    if len(audio_files) < 7:
        raise ValueError("éŸ³é¢‘æ–‡ä»¶ä¸è¶³7ä¸ªï¼Œæ— æ³•ç”Ÿæˆè§†é¢‘")

    # åŠ è½½æ‰€æœ‰éŸ³é¢‘å¹¶è®°å½•æ—¶é•¿
    audio_durations = []
    for i in range(7):  # åŠ è½½7ä¸ªéŸ³é¢‘
        audio_path = os.path.join(audio_dir, audio_files[i])
        audio_clip = AudioFileClip(audio_path)
        audio_durations.append(audio_clip.duration)
        audio_clips.append(audio_clip)

    def apply_effects(img_clip):
        """å¯¹å›¾ç‰‡å‰ªè¾‘åº”ç”¨ç‰¹æ•ˆ"""
        # è½»å¾®æ—‹è½¬å’Œç¼©æ”¾
        img_clip = img_clip.fx(vfx.rotate, angle=lambda t: 5 * np.sin(2 * np.pi * t / img_clip.duration), expand=False)
        img_clip = img_clip.fx(vfx.resize, lambda t: 1 + 0.1 * np.sin(2 * np.pi * t / img_clip.duration))

        # æ·¡å…¥æ·¡å‡ºæ•ˆæœ
        img_clip = img_clip.fx(vfx.fadein, 0.1).fx(vfx.fadeout, 0.1)

        return img_clip

    def apply_video_effects(video_clip):
        """å¯¹è§†é¢‘å‰ªè¾‘åº”ç”¨ç‰¹æ•ˆï¼Œåˆ†ä¸‰æ¬¡åº”ç”¨"""
        duration = video_clip.duration
        third_duration = duration / 3

        def apply_single_effect(clip):
            # è½»å¾®æ—‹è½¬å’Œç¼©æ”¾
            clip = clip.fx(vfx.rotate, angle=lambda t: 5 * np.sin(2 * np.pi * t / clip.duration), expand=False)
            clip = clip.fx(vfx.resize, lambda t: 1 + 0.1 * np.sin(2 * np.pi * t / clip.duration))
            return clip

        # åˆ†å‰²è§†é¢‘å‰ªè¾‘ä¸ºä¸‰ä¸ªéƒ¨åˆ†
        part1 = video_clip.subclip(0, third_duration)
        part2 = video_clip.subclip(third_duration, 2 * third_duration)
        part3 = video_clip.subclip(2 * third_duration, duration)

        # å¯¹æ¯ä¸ªéƒ¨åˆ†åº”ç”¨ç‰¹æ•ˆ
        part1 = apply_single_effect(part1)
        part2 = apply_single_effect(part2)
        part3 = apply_single_effect(part3)

        # åˆå¹¶ä¸‰ä¸ªéƒ¨åˆ†
        final_video_clip = concatenate_videoclips([part1, part2, part3])
        return final_video_clip

    # è§’è‰²1: 3å›¾ç‰‡å…±äº«éŸ³é¢‘1+2æ—¶é•¿ï¼Œè§†é¢‘ä½¿ç”¨éŸ³é¢‘3
    char1 = group_data[0]
    char1_audio_duration = audio_durations[0] + audio_durations[1]

    # è®¡ç®—æ¯å¼ å›¾ç‰‡çš„æ˜¾ç¤ºæ—¶é—´ï¼ˆå¹³å‡åˆ†é…ï¼‰
    if len(char1['image_files']) >= 3:
        per_image_duration = char1_audio_duration / 3
        for i in range(3):
            img_clip = ImageClip(char1['image_files'][i], duration=per_image_duration)
            img_clip = img_clip.resize(config['output_resolution'])
            img_clip = apply_effects(img_clip)
            clips.append(img_clip)
            current_time += per_image_duration

    # è§†é¢‘1ä½¿ç”¨éŸ³é¢‘3
    if len(char1['video_files']) > 0:
        video_clip = VideoFileClip(char1['video_files'][0])
        original_duration = video_clip.duration
        target_duration = audio_durations[2]
        speed_factor = original_duration / target_duration
        video_clip = video_clip.fx(vfx.speedx, speed_factor)
        video_clip = video_clip.resize(config['output_resolution'])
        video_clip = apply_video_effects(video_clip)
        clips.append(video_clip)
        current_time += target_duration

    # è§’è‰²2: 3å›¾ç‰‡å…±äº«éŸ³é¢‘4æ—¶é•¿ï¼Œè§†é¢‘ä½¿ç”¨éŸ³é¢‘5
    char2 = group_data[1]
    char2_audio_duration = audio_durations[3]

    if len(char2['image_files']) >= 3:
        per_image_duration = char2_audio_duration / 3
        for i in range(3):
            img_clip = ImageClip(char2['image_files'][i], duration=per_image_duration)
            img_clip = img_clip.resize(config['output_resolution'])
            img_clip = apply_effects(img_clip)
            clips.append(img_clip)
            current_time += per_image_duration

    # è§†é¢‘2ä½¿ç”¨éŸ³é¢‘5
    if len(char2['video_files']) > 0:
        video_clip = VideoFileClip(char2['video_files'][0])
        original_duration = video_clip.duration
        target_duration = audio_durations[4]
        speed_factor = original_duration / target_duration
        video_clip = video_clip.fx(vfx.speedx, speed_factor)
        video_clip = video_clip.resize(config['output_resolution'])
        video_clip = apply_video_effects(video_clip)
        clips.append(video_clip)
        current_time += target_duration

    # è§’è‰²3: 3å›¾ç‰‡å…±äº«éŸ³é¢‘6æ—¶é•¿ï¼Œè§†é¢‘ä½¿ç”¨éŸ³é¢‘7
    char3 = group_data[2]
    char3_audio_duration = audio_durations[5]

    if len(char3['image_files']) >= 3:
        per_image_duration = char3_audio_duration / 3
        for i in range(3):
            img_clip = ImageClip(char3['image_files'][i], duration=per_image_duration)
            img_clip = img_clip.resize(config['output_resolution'])
            img_clip = apply_effects(img_clip)
            clips.append(img_clip)
            current_time += per_image_duration

    # è§†é¢‘3ä½¿ç”¨éŸ³é¢‘7
    if len(char3['video_files']) > 0:
        video_clip = VideoFileClip(char3['video_files'][0])
        original_duration = video_clip.duration
        target_duration = audio_durations[6]
        speed_factor = original_duration / target_duration
        video_clip = video_clip.fx(vfx.speedx, speed_factor)
        video_clip = video_clip.resize(config['output_resolution'])
        video_clip = apply_video_effects(video_clip)
        clips.append(video_clip)
        current_time += target_duration

    # åˆå¹¶è§†é¢‘ç‰‡æ®µ
    final_video = concatenate_videoclips(clips, method="compose")

    # åˆå¹¶éŸ³é¢‘ï¼ˆä¸¥æ ¼å¯¹é½ï¼‰
    aligned_audio_clips = []
    current_audio_time = 0

    # è§’è‰²1éŸ³é¢‘ï¼ˆéŸ³é¢‘1+2ç”¨äºå›¾ç‰‡ï¼ŒéŸ³é¢‘3ç”¨äºè§†é¢‘ï¼‰
    aligned_audio_clips.append(audio_clips[0].set_start(current_audio_time))
    current_audio_time += audio_durations[0]
    aligned_audio_clips.append(audio_clips[1].set_start(current_audio_time))
    current_audio_time += audio_durations[1]
    aligned_audio_clips.append(audio_clips[2].set_start(current_audio_time))
    current_audio_time += audio_durations[2]

    # è§’è‰²2éŸ³é¢‘ï¼ˆéŸ³é¢‘4ç”¨äºå›¾ç‰‡ï¼ŒéŸ³é¢‘5ç”¨äºè§†é¢‘ï¼‰
    aligned_audio_clips.append(audio_clips[3].set_start(current_audio_time))
    current_audio_time += audio_durations[3]
    aligned_audio_clips.append(audio_clips[4].set_start(current_audio_time))
    current_audio_time += audio_durations[4]

    # è§’è‰²3éŸ³é¢‘ï¼ˆéŸ³é¢‘6ç”¨äºå›¾ç‰‡ï¼ŒéŸ³é¢‘7ç”¨äºè§†é¢‘ï¼‰
    aligned_audio_clips.append(audio_clips[5].set_start(current_audio_time))
    current_audio_time += audio_durations[5]
    aligned_audio_clips.append(audio_clips[6].set_start(current_audio_time))

    final_audio = CompositeAudioClip(aligned_audio_clips)
    final_video.audio = final_audio

    # å†™å…¥è¾“å‡ºæ–‡ä»¶
    final_video.write_videofile(output_path, fps=24, codec='libx264', audio_codec='aac')



def generate_videos(gender, num_groups, output_dir='output'):
    """ç”ŸæˆæŒ‡å®šæ•°é‡çš„è§†é¢‘"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # è·å–è§’è‰²ç»„åˆå’Œèµ„æº
    character_groups = get_character_resources(gender, num_groups)

    #print(character_groups)

    # ä¸ºæ¯ä¸ªç»„åˆåˆ›å»ºè§†é¢‘
    for i, group in enumerate(character_groups):
        output_path = os.path.join(output_dir, f'{gender}_group_{i+1}.mp4')
        print(f"æ­£åœ¨ç”Ÿæˆè§†é¢‘: {output_path}")
        print(f"è§’è‰²ç»„åˆ: {[char['name'] for char in group]}")

        try:
            create_video_clip(group, output_path)
            print(f"æˆåŠŸç”Ÿæˆ: {output_path}")
        except Exception as e:
            print(f"ç”Ÿæˆè§†é¢‘å¤±è´¥: {e}")
            continue

generate_videos('å¥³', 2)

generate_videos('ç”·', 2)
```

#### åŠ å…¥è§†é¢‘ç‰¹æ•ˆå’Œå­—å¹•æ‹¼æ¥ 
```python
import os
import random
import pandas as pd
from itertools import combinations
import subprocess
from moviepy.editor import concatenate_videoclips, AudioFileClip, ImageClip, CompositeAudioClip, VideoFileClip, TextClip, ColorClip, CompositeVideoClip
from moviepy.video.fx import all as vfx
from moviepy.video.fx.all import crop
from moviepy.audio.AudioClip import AudioClip
from PIL import Image
import numpy as np

# é…ç½®å‚æ•°
config = {
    'num_images_per_char': 3,  # æ¯ä¸ªè§’è‰²ä½¿ç”¨çš„å›¾ç‰‡æ•°é‡
    'output_resolution': (1024, 1024),  # è¾“å‡ºè§†é¢‘åˆ†è¾¨ç‡
    'image_duration': 3,  # æ¯å¼ å›¾ç‰‡æ˜¾ç¤ºæ—¶é•¿(ç§’)
    'video_duration': 5,  # æ¯ä¸ªè§†é¢‘ç‰‡æ®µæ—¶é•¿(ç§’)
    'audio_fade_duration': 0.5,  # éŸ³é¢‘æ·¡å…¥æ·¡å‡ºæ—¶é•¿(ç§’)
}

# è§’è‰²åå­—æ˜ å°„å’Œæ€§åˆ«æ˜ å°„
name_mapping = {
    'èŠ­èŠ­æ‹‰': 'BARBARA', 'æŸ¯è±': 'COLLEI', 'é›·ç”µå°†å†›': 'RAIDEN SHOGUN', 'äº‘å ‡': 'YUN JIN',
    'å…«é‡ç¥å­': 'YAE MIKO', 'å¦®éœ²': 'NILOU', 'ç»®è‰¯è‰¯': 'KIRARA', 'ç ‚ç³–': 'SUCROSE',
    'çéœ²çŠ': 'FARUZAN', 'ç³å¦®ç‰¹': 'LYNETTE', 'çº³è¥¿å¦²': 'NAHIDA', 'è¯ºè‰¾å°”': 'NOELLE',
    'å‡å…‰': 'NINGGUANG', 'é¹¿é‡é™¢å¹³è—': 'HEIZOU', 'ç´': 'JEAN', 'æ«åŸä¸‡å¶': 'KAEDEHARA KAZUHA',
    'èŠ™å®å¨œ': 'FURINA', 'è‰¾å°”æµ·æ£®': 'ALHAITHAM', 'ç”˜é›¨': 'GANYU', 'å‡¯äºš': 'KAEYA',
    'è’æ³·ä¸€æ–—': 'ARATAKI ITTO', 'ä¼˜èˆ': 'EULA', 'è¿ªå¥¥å¨œ': 'DIONA', 'æ¸©è¿ª': 'VENTI',
    'ç¥é‡Œç»«äºº': 'KAMISATO AYATO', 'é˜¿è´å¤š': 'ALBEDO', 'é‡äº‘': 'CHONGYUN', 'é’Ÿç¦»': 'ZHONGLI',
    'è¡Œç§‹': 'XINGQIU', 'èƒ¡æ¡ƒ': 'HU TAO', 'é­ˆ': 'XIAO', 'èµ›è¯º': 'CYNO',
    'ç¥é‡Œç»«å': 'KAMISATO AYAKA', 'äº”éƒ': 'GOROU', 'æ—å°¼': 'LYNEY', 'è¿ªå¢å…‹': 'DILUC',
    'å®‰æŸ': 'AMBER', 'çƒŸç»¯': 'YANFEI', 'å®µå®«': 'YOIMIYA', 'çŠç‘šå®«å¿ƒæµ·': 'SANGONOMIYA KOKOMI',
    'ç½—èè‰äºš': 'ROSARIA', 'ä¸ƒä¸ƒ': 'QIQI', 'ä¹…å²å¿': 'KUKI SHINOBU', 'ç”³é¹¤': 'SHENHE',
    'æ‰˜é©¬': 'THOMA', 'èŠ™å¯§å¨œ': 'FURINA', 'é›·æ³½': 'RAZOR'
}

gender_mapping = {
    'ä¹…å²å¿': 'å¥³', 'äº‘å ‡': 'å¥³', 'äº”éƒ': 'ç”·', 'ä¼˜èˆ': 'å¥³', 'å‡å…‰': 'å¥³', 'å‡¯äºš': 'ç”·',
    'å®‰æŸ': 'å¥³', 'å®µå®«': 'å¥³', 'æ¸©è¿ª': 'ç”·', 'çƒŸç»¯': 'å¥³', 'çŠç‘šå®«å¿ƒæµ·': 'å¥³', 'ç´': 'å¥³',
    'ç”˜é›¨': 'å¥³', 'ç”³é¹¤': 'å¥³', 'ç ‚ç³–': 'å¥³', 'ç¥é‡Œç»«äºº': 'ç”·', 'ç¥é‡Œç»«å': 'å¥³', 'ç»®è‰¯è‰¯': 'å¥³',
    'ç½—èè‰äºš': 'å¥³', 'èƒ¡æ¡ƒ': 'å¥³', 'è‰¾å°”æµ·æ£®': 'ç”·', 'è’æ³·ä¸€æ–—': 'ç”·', 'è¡Œç§‹': 'ç”·', 'è¯ºè‰¾å°”': 'å¥³',
    'è¿ªå¢å…‹': 'ç”·', 'è¿ªå¥¥å¨œ': 'å¥³', 'é‡äº‘': 'ç”·', 'é’Ÿç¦»': 'ç”·', 'é˜¿è´å¤š': 'ç”·', 'é›·æ³½': 'ç”·',
    'é›·ç”µå°†å†›': 'å¥³', 'é­ˆ': 'ç”·', 'é¹¿é‡é™¢å¹³è—': 'ç”·'
}

character_info = {
    'ä¹…å²å¿': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'é›·å…ƒç´ å¥¶å¦ˆï¼ˆä½†å¥¶é‡å…¨é é˜Ÿå‹è‡ªå·±åŠªåŠ›ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·å¥¶é˜Ÿå‹ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä»–ä»¬ä¹Ÿæ­»ä¸äº†ï¼‰'
    },
    'äº‘å ‡': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'ç’ƒæœˆæˆæ›²åè§’ï¼ˆä½†è§‚ä¼—ä¸»è¦æ˜¯ä¸ºäº†çœ‹è„¸ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·å”±æˆï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿå¬ä¸æ‡‚æˆè¯ï¼‰'
    },
    'äº”éƒ': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'æµ·ç¥‡å²›å¤§å°†ï¼ˆä½†æ‰“æ¶å…¨é ç‹—ç‹—å¸®å¿™ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·æ‰“ä»—ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿæ‰“ä¸è¿‡æˆ‘ï¼‰'
    },
    'ä¼˜èˆ': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'æµªèŠ±éª‘å£«ï¼ˆä½†æµªèŠ±ä¸»è¦æ˜¯ç”¨æ¥é€ƒè·‘çš„ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·å¤ä»‡ä¹‹èˆï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿè®°ä¸ä½ä»‡ï¼‰'
    },
    'å‡å…‰': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'å¤©æƒæ˜Ÿï¼ˆä½†é’±éƒ½ç”¨æ¥ä¹°æ–°è¡£æœäº†ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·èµšé’±ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿèµšä¸åˆ°æˆ‘çš„é’±ï¼‰'
    },
    'å‡¯äºš': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'æ¸¡æµ·çœŸå›ï¼ˆä½†æ¸¡æµ·ä¸»è¦é å†°é¢æ»‘è¡Œï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·å†°é¢æ»‘è¡Œï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿæ»‘ä¸è¿‡æˆ‘ï¼‰'
    },
    'å®‰æŸ': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'ä¾¦å¯Ÿéª‘å£«ï¼ˆä½†ä¾¦å¯Ÿä¸»è¦é å…”å…”ä¼¯çˆµï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·é£è¡Œå† å†›ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿé£ä¸è¿‡æˆ‘ï¼‰'
    },
    'å®µå®«': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'çƒŸèŠ±å¤§å¸ˆï¼ˆä½†çƒŸèŠ±ä¸»è¦æ˜¯ç”¨æ¥ç‚¸é±¼çš„ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·æ”¾çƒŸèŠ±ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿèº²ä¸å¼€æˆ‘çš„çƒŸèŠ±ï¼‰'
    },
    'æ¸©è¿ª': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'åŸæ¸¸è¯—äººï¼ˆä½†ä¸»è¦æ”¶å…¥æ¥æºæ˜¯è¹­é…’ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·å–é…’ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿå–ä¸è¿‡æˆ‘ï¼‰'
    },
    'çƒŸç»¯': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'å¾‹æ³•ä¸“å®¶ï¼ˆä½†æ‰“å®˜å¸ä¸»è¦é å˜´ç‚®ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·æ‰“å®˜å¸ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿè¯´ä¸è¿‡æˆ‘ï¼‰'
    },
    'çŠç‘šå®«å¿ƒæµ·': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'ç°äººç¥å·«å¥³ï¼ˆä½†å†›äº‹ç­–ç•¥å…¨é é”¦å›Šï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·å†›äº‹ç­–ç•¥ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿçœ‹ä¸æ‡‚é”¦å›Šï¼‰'
    },
    'ç´': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'è’²å…¬è‹±éª‘å£«ï¼ˆä½†ä¸»è¦å·¥ä½œæ˜¯æ‰¹æ–‡ä»¶ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·æ‰¹æ–‡ä»¶ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿæ‰¹ä¸å®Œï¼‰'
    },
    'ç”˜é›¨': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'éº’éºŸå°‘å¥³ï¼ˆä½†åŠ ç­åŠ åˆ°å¿˜è®°è‡ªå·±æ˜¯éº’éºŸï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·åŠ ç­ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹ŸåŠ ä¸å®Œï¼‰'
    },
    'ç”³é¹¤': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'é©±é‚ªæ–¹å£«ï¼ˆä½†é©±é‚ªä¸»è¦é ç‰©ç†è¶…åº¦ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·é™¤å¦–ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿæ‰“ä¸è¿‡æˆ‘ï¼‰'
    },
    'ç ‚ç³–': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'ç‚¼é‡‘æœ¯å£«ï¼ˆä½†å®éªŒä¸»è¦é è¿æ°”ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·åšå®éªŒï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿçœ‹ä¸æ‡‚é…æ–¹ï¼‰'
    },
    'ç¥é‡Œç»«äºº': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'ç¤¾å¥‰è¡Œå®¶ä¸»ï¼ˆä½†å·¥ä½œä¸»è¦é å¦¹å¦¹å¸®å¿™ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·å¤„ç†æ”¿åŠ¡ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿå¤„ç†ä¸å®Œï¼‰'
    },
    'ç¥é‡Œç»«å': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'ç™½é¹­å…¬ä¸»ï¼ˆä½†å‰‘æœ¯è¡¨æ¼”ä¸»è¦ä¸ºäº†å¥½çœ‹ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·å‰‘æœ¯è¡¨æ¼”ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿå­¦ä¸ä¼šï¼‰'
    },
    'ç»®è‰¯è‰¯': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'å¿«é€’å‘˜ï¼ˆä½†é€è´§ä¸»è¦é æ»šæ¥æ»šå»ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·é€å¿«é€’ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿè¿½ä¸ä¸Šæˆ‘ï¼‰'
    },
    'ç½—èè‰äºš': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'ä¿®å¥³ï¼ˆä½†ç¥·å‘Šæ—¶é—´ä¸»è¦ç”¨æ¥ç¡è§‰ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·å¤œé—´å·¡é€»ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿæ‰¾ä¸åˆ°æˆ‘ï¼‰'
    },
    'èƒ¡æ¡ƒ': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'å¾€ç”Ÿå ‚å ‚ä¸»ï¼ˆä½†æ¨é”€æ£ºæä¸»è¦é æŠ¼éŸµï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·æ¨é”€æ£ºæï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿé€ƒä¸æ‰ï¼‰'
    },
    'è‰¾å°”æµ·æ£®': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'ä¹¦è®°å®˜ï¼ˆä½†çœ‹ä¹¦ä¸»è¦ä¸ºäº†æŠ¬æ ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·çœ‹ä¹¦ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿè¯´ä¸è¿‡æˆ‘ï¼‰'
    },
    'è’æ³·ä¸€æ–—': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'é¬¼æ—è±ªæ°ï¼ˆä½†æ‰“æ¶ä¸»è¦é å—“é—¨å¤§ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·ç›¸æ‰‘æ¯”èµ›ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿèµ¢ä¸äº†ï¼‰'
    },
    'è¡Œç§‹': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'é£äº‘å•†ä¼šäºŒå°å§ï¼ˆä½†å†™å°è¯´ä¸»è¦é è„‘è¡¥ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·çœ‹æ­¦ä¾ å°è¯´ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿå†™ä¸å‡ºæ¥ï¼‰'
    },
    'è¯ºè‰¾å°”': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'å¥³ä»†éª‘å£«ï¼ˆä½†æ‰“æ‰«èŒƒå›´åŒ…æ‹¬æ•´ä¸ªè’™å¾·ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·æ‰“æ‰«å«ç”Ÿï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿæ‹¦ä¸ä½æˆ‘ï¼‰'
    },
    'è¿ªå¢å…‹': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'æš—å¤œè‹±é›„ï¼ˆä½†è¡Œä¾ ä¸»è¦é é’èƒ½åŠ›ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·æ‰“å‡»çŠ¯ç½ªï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿä¹°ä¸èµ·é…’åº„ï¼‰'
    },
    'è¿ªå¥¥å¨œ': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'çŒ«å°¾é…’ä¿ï¼ˆä½†è°ƒé…’ä¸»è¦ä¸ºäº†éš¾å–ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·è°ƒé…’ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿä¸æ•¢å–ï¼‰'
    },
    'é‡äº‘': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'é©±é‚ªä¸–å®¶ä¼ äººï¼ˆä½†æœ€æ€•åƒè¾£ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·åƒå†°æ£ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿå¿ä¸ä½ï¼‰'
    },
    'é’Ÿç¦»': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'å¾€ç”Ÿå ‚å®¢å¿ï¼ˆä½†è®°è´¦ä¸»è¦é å…¬å­ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·å¬æˆï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿä»˜ä¸èµ·é’±ï¼‰'
    },
    'é˜¿è´å¤š': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'ç™½å©ä¹‹å­ï¼ˆä½†ç”»ç”»ä¸»è¦é ç‚¼é‡‘æœ¯ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·ç”»ç”»ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿçœ‹ä¸æ‡‚ï¼‰'
    },
    'é›·æ³½': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'ç‹¼å°‘å¹´ï¼ˆä½†è¯´è¯ä¸»è¦é å¢çš®å¡ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·å’Œç‹¼ç¾¤ç©è€ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿå¬ä¸æ‡‚ï¼‰'
    },
    'é›·ç”µå°†å†›': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'å¾¡å»ºé¸£ç¥ä¸»å°Šå¤§å¾¡æ‰€å¤§äººï¼ˆä½†åšé¥­ä¼šå¼•å‘æ ¸çˆ†ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·è¿½æ±‚æ°¸æ’ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿé€ƒä¸æ‰ï¼‰'
    },
    'é­ˆ': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'æŠ¤æ³•å¤œå‰ï¼ˆä½†æ€»åœ¨å±‹é¡¶çœ‹é£æ™¯ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·é™¤é­”ï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹Ÿæ‰¾ä¸åˆ°æˆ‘ï¼‰'
    },
    'é¹¿é‡é™¢å¹³è—': {
        '[å¯¹è§’è‰²çš„ç§°å‘¼]': 'å¤©é¢†å¥‰è¡Œä¾¦æ¢ï¼ˆä½†ç ´æ¡ˆä¸»è¦é ç›´è§‰ï¼‰',
        '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': 'æ¯”èµ·ç ´æ¡ˆï¼Œæˆ‘æ›´å–œæ¬¢ä½ ï¼ˆåæ­£ä½ ä¹ŸçŒœä¸é€ï¼‰'
    }
}


# è·¯å¾„é…ç½®
paths = {
    'female_audios': 'I_prefer_you_over_something_GIRL_AUDIOS_SPLITED',
    'male_audios': 'I_prefer_you_over_something_BOY_AUDIOS_SPLITED',
    'images': 'Genshin-Impact-Portrait-with-Tags-Filtered-IID-Gender-SP',
    'videos': 'Genshin_Impact_Birthday_Art_FramePack_Rotate_Named'
}

# åŠ è½½è§†é¢‘å…ƒæ•°æ®
video_metadata = pd.read_csv(os.path.join(paths['videos'], 'metadata.csv'))

def get_character_resources(gender, num_groups):
    """è·å–æŒ‡å®šæ€§åˆ«çš„è§’è‰²ç»„åˆå’Œèµ„æº"""
    # æŒ‰æ€§åˆ«ç­›é€‰è§’è‰²
    chars = [name for name, g in gender_mapping.items() if g == gender]

    # ç”Ÿæˆæ‰€æœ‰3è§’è‰²ç»„åˆ
    all_combinations = list(combinations(chars, 3))
    random.shuffle(all_combinations)

    # åªå–éœ€è¦çš„æ•°é‡
    selected_combinations = all_combinations[:num_groups]

    results = []
    for combo in selected_combinations:
        group_data = []
        for char in combo:
            # è·å–è§’è‰²è‹±æ–‡å
            en_name = name_mapping.get(char, char).replace(' ', '_').upper()

            # è·å–éŸ³é¢‘æ–‡ä»¶
            audio_dir = paths['female_audios'] if gender == 'å¥³' else paths['male_audios']
            audio_files = sorted([f for f in os.listdir(audio_dir) if f.endswith('.wav') or f.endswith('.mp3')])

            # è·å–å›¾ç‰‡æ–‡ä»¶
            image_dir = os.path.join(paths['images'], f'genshin_impact_{en_name}_images_and_texts')
            image_files = []
            if os.path.exists(image_dir):
                image_files = [f for f in os.listdir(image_dir) if f.lower().endswith('.png')]
                random.shuffle(image_files)
                image_files = image_files[:config['num_images_per_char']]

            # è·å–è§†é¢‘æ–‡ä»¶
            video_files = video_metadata[video_metadata['prompt'] == char]['file_name'].tolist()

            group_data.append({
                'name': char,
                'en_name': en_name,
                'audio_files': audio_files,
                'image_files': [os.path.join(image_dir, f) for f in image_files],
                'video_files': [os.path.join(paths['videos'], f) for f in video_files]
            })

        results.append(group_data)

    return results

def create_video_clip(group_data, output_path):
    """ä¸ºå•ä¸ªè§’è‰²ç»„åˆåˆ›å»ºè§†é¢‘ï¼ˆæ ¹æ®éŸ³é¢‘æ—¶é—´åˆ†é…å›¾ç‰‡æ˜¾ç¤ºæ—¶é—´ï¼‰ï¼Œå¹¶ç”Ÿæˆç‹¬ç«‹çš„SRTå­—å¹•æ–‡ä»¶"""
    clips = []
    audio_clips = []
    current_time = 0  # è·Ÿè¸ªå½“å‰æ—¶é—´ä½ç½®

    # è·å–è§’è‰²åå­—ç”¨äºè¾“å‡ºæ–‡ä»¶å
    char_names = "_".join([char['name'] for char in group_data])
    base_output_path = output_path.replace('.mp4', f'_{char_names}.mp4')
    srt_output_path = base_output_path.replace('.mp4', '.srt')  # å­—å¹•æ–‡ä»¶è·¯å¾„

    # è·å–éŸ³é¢‘æ–‡ä»¶
    audio_dir = paths['female_audios'] if gender_mapping[group_data[0]['name']] == 'å¥³' else paths['male_audios']
    audio_files = sorted([f for f in os.listdir(audio_dir) if f.endswith(('.wav', '.mp3'))])

    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„éŸ³é¢‘æ–‡ä»¶ï¼ˆ7ä¸ªï¼‰
    if len(audio_files) < 7:
        raise ValueError("éŸ³é¢‘æ–‡ä»¶ä¸è¶³7ä¸ªï¼Œæ— æ³•ç”Ÿæˆè§†é¢‘")

    # åŠ è½½æ‰€æœ‰éŸ³é¢‘å¹¶è®°å½•æ—¶é•¿
    audio_durations = []
    for i in range(7):  # åŠ è½½7ä¸ªéŸ³é¢‘
        audio_path = os.path.join(audio_dir, audio_files[i])
        audio_clip = AudioFileClip(audio_path)
        audio_durations.append(audio_clip.duration)
        audio_clips.append(audio_clip)

    # ç”Ÿæˆå­—å¹•å†…å®¹
    srt_template = """
1
00:00:00,000 --> 00:00:02,050
{char1_title}

2
00:00:02,050 --> 00:00:04,050
ä½ å–œæ¬¢ä»€ä¹ˆï¼Ÿ

3
00:00:04,050 --> 00:00:07,100
{char1_preference}

4
00:00:07,100 --> 00:00:11,100
{char2_title}ï¼Œä½ å–œæ¬¢ä»€ä¹ˆï¼Ÿ

5
00:00:11,100 --> 00:00:14,300
{char2_preference}

6
00:00:14,300 --> 00:00:18,150
{char3_title}ï¼Œä½ å–œæ¬¢ä»€ä¹ˆï¼Ÿ

7
00:00:18,150 --> 00:00:23,000
{char3_preference}

8
00:00:23,000 --> 00:00:23,000
{char3_preference}
"""

    # å¡«å……å­—å¹•æ¨¡æ¿
    char1_info = character_info.get(group_data[0]['name'], {'[å¯¹è§’è‰²çš„ç§°å‘¼]': group_data[0]['name'], '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': f'æ¯”èµ·XXXï¼Œæˆ‘æ›´å–œæ¬¢ä½ '})
    char2_info = character_info.get(group_data[1]['name'], {'[å¯¹è§’è‰²çš„ç§°å‘¼]': group_data[1]['name'], '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': f'æ¯”èµ·XXXï¼Œæˆ‘æ›´å–œæ¬¢ä½ '})
    char3_info = character_info.get(group_data[2]['name'], {'[å¯¹è§’è‰²çš„ç§°å‘¼]': group_data[2]['name'], '[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]': f'æ¯”èµ·XXXï¼Œæˆ‘æ›´å–œæ¬¢ä½ '})

    srt_content = srt_template.format(
        char1_title=char1_info['[å¯¹è§’è‰²çš„ç§°å‘¼]'],
        char1_preference=char1_info['[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]'],
        char2_title=char2_info['[å¯¹è§’è‰²çš„ç§°å‘¼]'],
        char2_preference=char2_info['[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]'],
        char3_title=char3_info['[å¯¹è§’è‰²çš„ç§°å‘¼]'],
        char3_preference=char3_info['[æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½ ]']
    ).strip()

    # å°†å­—å¹•å†…å®¹å†™å…¥SRTæ–‡ä»¶
    with open(srt_output_path, 'w', encoding='utf-8') as f:
        f.write(srt_content)

    def apply_effects(img_clip):
        """å¯¹å›¾ç‰‡å‰ªè¾‘åº”ç”¨ç‰¹æ•ˆ"""
        # è½»å¾®æ—‹è½¬å’Œç¼©æ”¾
        img_clip = img_clip.fx(vfx.rotate, angle=lambda t: 5 * np.sin(2 * np.pi * t / img_clip.duration), expand=False)
        img_clip = img_clip.fx(vfx.resize, lambda t: 1 + 0.1 * np.sin(2 * np.pi * t / img_clip.duration))

        # æ·¡å…¥æ·¡å‡ºæ•ˆæœ
        img_clip = img_clip.fx(vfx.fadein, 0.1).fx(vfx.fadeout, 0.1)

        return img_clip

    def apply_video_effects(video_clip):
        """å¯¹è§†é¢‘å‰ªè¾‘åº”ç”¨ç‰¹æ•ˆï¼Œåˆ†ä¸‰æ¬¡åº”ç”¨"""
        duration = video_clip.duration
        third_duration = duration / 3

        def apply_single_effect(clip):
            # è½»å¾®æ—‹è½¬å’Œç¼©æ”¾
            clip = clip.fx(vfx.rotate, angle=lambda t: 5 * np.sin(2 * np.pi * t / clip.duration), expand=False)
            clip = clip.fx(vfx.resize, lambda t: 1 + 0.1 * np.sin(2 * np.pi * t / clip.duration))
            return clip

        # åˆ†å‰²è§†é¢‘å‰ªè¾‘ä¸ºä¸‰ä¸ªéƒ¨åˆ†
        part1 = video_clip.subclip(0, third_duration)
        part2 = video_clip.subclip(third_duration, 2 * third_duration)
        part3 = video_clip.subclip(2 * third_duration, duration)

        # å¯¹æ¯ä¸ªéƒ¨åˆ†åº”ç”¨ç‰¹æ•ˆ
        part1 = apply_single_effect(part1)
        part2 = apply_single_effect(part2)
        part3 = apply_single_effect(part3)

        # åˆå¹¶ä¸‰ä¸ªéƒ¨åˆ†
        final_video_clip = concatenate_videoclips([part1, part2, part3])
        return final_video_clip

    # è§’è‰²1: 3å›¾ç‰‡å…±äº«éŸ³é¢‘1+2æ—¶é•¿ï¼Œè§†é¢‘ä½¿ç”¨éŸ³é¢‘3
    char1 = group_data[0]
    char1_audio_duration = audio_durations[0] + audio_durations[1]

    # è®¡ç®—æ¯å¼ å›¾ç‰‡çš„æ˜¾ç¤ºæ—¶é—´ï¼ˆå¹³å‡åˆ†é…ï¼‰
    if len(char1['image_files']) >= 3:
        per_image_duration = char1_audio_duration / 3
        for i in range(3):
            img_clip = ImageClip(char1['image_files'][i], duration=per_image_duration)
            img_clip = img_clip.resize(config['output_resolution'])
            img_clip = apply_effects(img_clip)
            clips.append(img_clip)
            current_time += per_image_duration

    # è§†é¢‘1ä½¿ç”¨éŸ³é¢‘3
    if len(char1['video_files']) > 0:
        video_clip = VideoFileClip(char1['video_files'][0])
        original_duration = video_clip.duration
        target_duration = audio_durations[2]
        speed_factor = original_duration / target_duration
        video_clip = video_clip.fx(vfx.speedx, speed_factor)
        video_clip = video_clip.resize(config['output_resolution'])
        video_clip = apply_video_effects(video_clip)
        clips.append(video_clip)
        current_time += target_duration

    # è§’è‰²2: 3å›¾ç‰‡å…±äº«éŸ³é¢‘4æ—¶é•¿ï¼Œè§†é¢‘ä½¿ç”¨éŸ³é¢‘5
    char2 = group_data[1]
    char2_audio_duration = audio_durations[3]

    if len(char2['image_files']) >= 3:
        per_image_duration = char2_audio_duration / 3
        for i in range(3):
            img_clip = ImageClip(char2['image_files'][i], duration=per_image_duration)
            img_clip = img_clip.resize(config['output_resolution'])
            img_clip = apply_effects(img_clip)
            clips.append(img_clip)
            current_time += per_image_duration

    # è§†é¢‘2ä½¿ç”¨éŸ³é¢‘5
    if len(char2['video_files']) > 0:
        video_clip = VideoFileClip(char2['video_files'][0])
        original_duration = video_clip.duration
        target_duration = audio_durations[4]
        speed_factor = original_duration / target_duration
        video_clip = video_clip.fx(vfx.speedx, speed_factor)
        video_clip = video_clip.resize(config['output_resolution'])
        video_clip = apply_video_effects(video_clip)
        clips.append(video_clip)
        current_time += target_duration

    # è§’è‰²3: 3å›¾ç‰‡å…±äº«éŸ³é¢‘6æ—¶é•¿ï¼Œè§†é¢‘ä½¿ç”¨éŸ³é¢‘7
    char3 = group_data[2]
    char3_audio_duration = audio_durations[5]

    if len(char3['image_files']) >= 3:
        per_image_duration = char3_audio_duration / 3
        for i in range(3):
            img_clip = ImageClip(char3['image_files'][i], duration=per_image_duration)
            img_clip = img_clip.resize(config['output_resolution'])
            img_clip = apply_effects(img_clip)
            clips.append(img_clip)
            current_time += per_image_duration

    # è§†é¢‘3ä½¿ç”¨éŸ³é¢‘7
    if len(char3['video_files']) > 0:
        video_clip = VideoFileClip(char3['video_files'][0])
        original_duration = video_clip.duration
        target_duration = audio_durations[6]
        speed_factor = original_duration / target_duration
        video_clip = video_clip.fx(vfx.speedx, speed_factor)
        video_clip = video_clip.resize(config['output_resolution'])
        video_clip = apply_video_effects(video_clip)
        clips.append(video_clip)
        current_time += target_duration

    # åˆå¹¶è§†é¢‘ç‰‡æ®µ
    final_video = concatenate_videoclips(clips, method="compose")

    # åˆå¹¶éŸ³é¢‘ï¼ˆä¸¥æ ¼å¯¹é½ï¼‰
    aligned_audio_clips = []
    current_audio_time = 0

    # è§’è‰²1éŸ³é¢‘ï¼ˆéŸ³é¢‘1+2ç”¨äºå›¾ç‰‡ï¼ŒéŸ³é¢‘3ç”¨äºè§†é¢‘ï¼‰
    aligned_audio_clips.append(audio_clips[0].set_start(current_audio_time))
    current_audio_time += audio_durations[0]
    aligned_audio_clips.append(audio_clips[1].set_start(current_audio_time))
    current_audio_time += audio_durations[1]
    aligned_audio_clips.append(audio_clips[2].set_start(current_audio_time))
    current_audio_time += audio_durations[2]

    # è§’è‰²2éŸ³é¢‘ï¼ˆéŸ³é¢‘4ç”¨äºå›¾ç‰‡ï¼ŒéŸ³é¢‘5ç”¨äºè§†é¢‘ï¼‰
    aligned_audio_clips.append(audio_clips[3].set_start(current_audio_time))
    current_audio_time += audio_durations[3]
    aligned_audio_clips.append(audio_clips[4].set_start(current_audio_time))
    current_audio_time += audio_durations[4]

    # è§’è‰²3éŸ³é¢‘ï¼ˆéŸ³é¢‘6ç”¨äºå›¾ç‰‡ï¼ŒéŸ³é¢‘7ç”¨äºè§†é¢‘ï¼‰
    aligned_audio_clips.append(audio_clips[5].set_start(current_audio_time))
    current_audio_time += audio_durations[5]
    aligned_audio_clips.append(audio_clips[6].set_start(current_audio_time))

    final_audio = CompositeAudioClip(aligned_audio_clips)
    final_video.audio = final_audio

    # å†™å…¥è¾“å‡ºæ–‡ä»¶ï¼ˆä¸å¸¦å­—å¹•ï¼‰
    final_video.write_videofile(base_output_path, fps=24, codec='libx264', audio_codec='aac')

def convert_srt_time_to_seconds(time_str):
    """å°†SRTæ—¶é—´æ ¼å¼è½¬æ¢ä¸ºç§’"""
    h, m, s = time_str.split(':')
    s, ms = s.split(',')
    return int(h)*3600 + int(m)*60 + int(s) + int(ms)/1000

def generate_videos(gender, num_groups, output_dir='output'):
    """ç”ŸæˆæŒ‡å®šæ•°é‡çš„è§†é¢‘"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # è·å–è§’è‰²ç»„åˆå’Œèµ„æº
    character_groups = get_character_resources(gender, num_groups)

    #print(character_groups)

    # ä¸ºæ¯ä¸ªç»„åˆåˆ›å»ºè§†é¢‘
    for i, group in enumerate(character_groups):
        output_path = os.path.join(output_dir, f'{gender}_group_{i+1}.mp4')
        print(f"æ­£åœ¨ç”Ÿæˆè§†é¢‘: {output_path}")
        print(f"è§’è‰²ç»„åˆ: {[char['name'] for char in group]}")

        try:
            create_video_clip(group, output_path)
            print(f"æˆåŠŸç”Ÿæˆ: {output_path}")
        except Exception as e:
            print(f"ç”Ÿæˆè§†é¢‘å¤±è´¥: {e}")
            continue

generate_videos('å¥³', 2, output_dir = "Genshin_Impact_Girls_prefer_you_over_OTHERS")

generate_videos('ç”·', 2, output_dir = "Genshin_Impact_Boys_prefer_you_over_OTHERS")


import os
from pathlib import Path
from moviepy.editor import VideoFileClip, TextClip, CompositeVideoClip
from moviepy.video.tools.subtitles import SubtitlesClip

def add_subtitles_with_moviepy(input_path, output_path, font="simhei.ttf", font_size=54, color="white", bg_color="black", stroke_color="black", stroke_width=1):
    """
    ä½¿ç”¨ moviepy å°† SRT å­—å¹•æ·»åŠ åˆ° MP4 è§†é¢‘åº•éƒ¨ï¼Œå¹¶è¾“å‡ºåˆ°æŒ‡å®šæ–‡ä»¶å¤¹ã€‚

    å‚æ•°:
        input_path (str): è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆåŒ…å« .mp4 å’Œ .srt æ–‡ä»¶ï¼‰
        output_path (str): è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
        font (str): å­—ä½“ï¼ˆé»˜è®¤ "Arial"ï¼‰
        font_size (int): å­—ä½“å¤§å°ï¼ˆé»˜è®¤ 24ï¼‰
        color (str): å­—ä½“é¢œè‰²ï¼ˆé»˜è®¤ "white"ï¼‰
        bg_color (str): èƒŒæ™¯é¢œè‰²ï¼ˆé»˜è®¤ "black"ï¼‰
        stroke_color (str): æè¾¹é¢œè‰²ï¼ˆé»˜è®¤ "black"ï¼‰
        stroke_width (int): æè¾¹å®½åº¦ï¼ˆé»˜è®¤ 1ï¼‰
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path(output_path).mkdir(parents=True, exist_ok=True)

    # æ‰«æè¾“å…¥ç›®å½•ä¸‹çš„ .mp4 å’Œ .srt æ–‡ä»¶
    for root, _, files in os.walk(input_path):
        for file in files:
            if file.lower().endswith(".mp4"):
                video_path = os.path.join(root, file)
                base_name = os.path.splitext(file)[0]
                srt_path = os.path.join(root, f"{base_name}.srt")

                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¯¹åº”çš„ .srt æ–‡ä»¶
                if not os.path.exists(srt_path):
                    print(f"âš ï¸ æœªæ‰¾åˆ° {base_name}.srt å­—å¹•æ–‡ä»¶ï¼Œè·³è¿‡ {file}")
                    continue

                # åŠ è½½è§†é¢‘
                video = VideoFileClip(video_path)

                '''
                # æ£€æŸ¥åˆ†è¾¨ç‡æ˜¯å¦ä¸º 1024x1024
                if video.size != (1024, 1024):
                    print(f"âš ï¸ {file} ä¸æ˜¯ 1024x1024 åˆ†è¾¨ç‡ï¼Œè·³è¿‡")
                    video.close()
                    continue
                '''

                # åŠ è½½å­—å¹•ï¼ˆä½¿ç”¨ moviepy çš„ SubtitlesClipï¼‰
                def make_text(txt):
                    """ç”Ÿæˆå­—å¹•æ ·å¼"""

                    text_clip = TextClip(
                        txt,
                        font=font,
                        fontsize=font_size,
                        color=color,
                        bg_color=bg_color,
                        stroke_color=stroke_color,
                        stroke_width=stroke_width,
                        size=(video.w, None),  # å®½åº¦ä¸è§†é¢‘ç›¸åŒï¼Œé«˜åº¦è‡ªåŠ¨è°ƒæ•´
                        method="caption",  # è‡ªåŠ¨æ¢è¡Œ
                        align="center",  # æ°´å¹³å±…ä¸­
                    )

                    # è®¡ç®—å­—å¹•ä½ç½®ï¼ˆè·ç¦»åº•éƒ¨ margin åƒç´ ï¼‰
                    margin = 130  # è°ƒæ•´è¿™ä¸ªå€¼æ¥æ§åˆ¶ä¸åº•éƒ¨çš„è·ç¦»
                    text_position = ("center", video.h - text_clip.h - margin)

                    # è®¾ç½®ä½ç½®
                    text_clip = text_clip.set_position(text_position)
                    return text_clip

                subtitles = SubtitlesClip(srt_path, make_text)

                # åˆæˆè§†é¢‘+å­—å¹•
                final_video = CompositeVideoClip([video, subtitles.set_position(("center", "bottom"))])

                # è¾“å‡ºæ–‡ä»¶è·¯å¾„
                output_file = os.path.join(output_path, f"{base_name}_subtitled.mp4")

                # å†™å…¥è¾“å‡ºæ–‡ä»¶
                final_video.write_videofile(
                    output_file,
                    codec="libx264",  # H.264 ç¼–ç 
                    audio_codec="aac",  # ä¿æŒåŸéŸ³é¢‘
                    fps=video.fps,  # ä¿æŒåŸå¸§ç‡
                    threads=4,  # å¤šçº¿ç¨‹åŠ é€Ÿ
                )

                print(f"âœ… å­—å¹•å·²æ·»åŠ ï¼š{output_file}")

                # é‡Šæ”¾èµ„æº
                video.close()
                final_video.close()

# ç¤ºä¾‹è°ƒç”¨
if __name__ == "__main__":
    input_dir = "Genshin_Impact_Girls_prefer_you_over_OTHERS"  # æ›¿æ¢ä¸ºä½ çš„è¾“å…¥è·¯å¾„
    output_dir = "Genshin_Impact_Girls_prefer_you_over_OTHERS_Subtitled"  # æ›¿æ¢ä¸ºä½ çš„è¾“å‡ºè·¯å¾„
    add_subtitles_with_moviepy(input_dir, output_dir)


# ç¤ºä¾‹è°ƒç”¨
if __name__ == "__main__":
    input_dir = "Genshin_Impact_Boys_prefer_you_over_OTHERS"  # æ›¿æ¢ä¸ºä½ çš„è¾“å…¥è·¯å¾„
    output_dir = "Genshin_Impact_Boys_prefer_you_over_OTHERS_Subtitled"  # æ›¿æ¢ä¸ºä½ çš„è¾“å‡ºè·¯å¾„
    add_subtitles_with_moviepy(input_dir, output_dir)

```
