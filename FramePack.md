```bash

sudo apt-get update && sudo apt-get install git-lfs ffmpeg cbm 

git clone -b frame-pack-inference https://github.com/kohya-ss/musubi-tuner.git
cd musubi-tuner

pip install torch torchvision
pip install -r requirements.txt
pip install ascii-magic matplotlib tensorboard huggingface_hub datasets
pip install moviepy==1.0.3
pip install sageattention==1.0.6

git clone https://huggingface.co/lllyasviel/FramePackI2V_HY
->
--dit FramePackI2V_HY/diffusion_pytorch_model-00001-of-00003.safetensors

huggingface-cli download tencent/HunyuanVideo --local-dir ./ckpts
->
ckpts/hunyuan-video-t2v-720p/vae/pytorch_model.pt
OR 
git clone https://huggingface.co/hunyuanvideo-community/HunyuanVideo
->
--vae HunyuanVideo/vae/diffusion_pytorch_model.safetensors

git clone https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged
->
--text_encoder1 HunyuanVideo_repackaged/split_files/text_encoders/llava_llama3_fp16.safetensors
--text_encoder2 HunyuanVideo_repackaged/split_files/text_encoders/clip_l.safetensors 

git clone https://huggingface.co/Comfy-Org/sigclip_vision_384
->
--image_encoder sigclip_vision_384/sigclip_vision_patch14_384.safetensors


```

#### ç¤ºä¾‹è®­ç»ƒ
```python
#### bigger than 37
import os
import shutil
from moviepy.editor import VideoFileClip

# å®šä¹‰è·¯å¾„
src_dir = "Yi_Chen_Dancing_Animation_Videos_White_Background_Splited_Captioned_960x544x6"
dst_dir = "Yi_Chen_Dancing_Animation_Videos_White_Background_Splited_Captioned_960x544x6_upper_60fm"

# åˆ›å»ºç›®æ ‡ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
os.makedirs(dst_dir, exist_ok=True)

# éå†æºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
for filename in os.listdir(src_dir):
    if filename.endswith(".mp4"):
        mp4_path = os.path.join(src_dir, filename)
        txt_path = os.path.join(src_dir, filename.replace(".mp4", ".txt"))

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¯¹åº”çš„.txtæ–‡ä»¶
        if not os.path.exists(txt_path):
            print(f"è­¦å‘Š: æœªæ‰¾åˆ° {filename} å¯¹åº”çš„ .txt æ–‡ä»¶ï¼Œè·³è¿‡ã€‚")
            continue

        # ä½¿ç”¨MoviePyè·å–è§†é¢‘å¸§æ•°
        try:
            with VideoFileClip(mp4_path) as video:
                frame_count = int(video.duration * video.fps)
                print(f"å¤„ç†: {filename} | å¸§æ•°: {frame_count}")

                # å¦‚æœå¸§æ•°>60ï¼Œæ‹·è´æ–‡ä»¶å¯¹
                if frame_count > 60:
                    shutil.copy2(mp4_path, dst_dir)
                    shutil.copy2(txt_path, dst_dir)
                    print(f"å·²æ‹·è´: {filename} å’ŒåŒå .txt æ–‡ä»¶åˆ° {dst_dir}")
        except Exception as e:
            print(f"é”™è¯¯: å¤„ç† {filename} æ—¶å‡ºé”™ - {str(e)}")

print("å¤„ç†å®Œæˆï¼")
```

```toml
[general]
resolution = [960, 544]
caption_extension = ".txt"
batch_size = 1
enable_bucket = true
bucket_no_upscale = false

[[datasets]]
video_directory = "Yi_Chen_Dancing_Animation_Videos_White_Background_Splited_Captioned_960x544x6_upper_60fm"
cache_directory = "Yi_Chen_Dancing_Animation_Videos_White_Background_Splited_Captioned_960x544x6_upper_60fm_cache"
target_frames = [1, 25, 79]
max_frames = 129
frame_extraction = "full"
```

```toml
[general]
resolution = [960, 544]
caption_extension = ".txt"
batch_size = 1
enable_bucket = true
bucket_no_upscale = false

[[datasets]]
video_directory = "Yi_Chen_Dancing_Animation_Videos_White_Background_Splited_Captioned_960x544x6_upper_60fm"
cache_directory = "Yi_Chen_Dancing_Animation_Videos_White_Background_Splited_Captioned_960x544x6_upper_60fm_45_cache"
target_frames = [1, 25, 45]
max_frames = 45
frame_extraction = "full"
```

```bash
python fpack_cache_latents.py \
    --dataset_config video_config.toml \
    --vae HunyuanVideo/vae/diffusion_pytorch_model.safetensors \
    --image_encoder sigclip_vision_384/sigclip_vision_patch14_384.safetensors \
    --vae_chunk_size 32 --vae_spatial_tile_sample_min_size 128

python fpack_cache_text_encoder_outputs.py \
    --dataset_config video_config.toml \
    --text_encoder1 HunyuanVideo_repackaged/split_files/text_encoders/llava_llama3_fp16.safetensors \
    --text_encoder2 HunyuanVideo_repackaged/split_files/text_encoders/clip_l.safetensors \
    --batch_size 16

accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 fpack_train_network.py \
    --dit FramePackI2V_HY/diffusion_pytorch_model-00001-of-00003.safetensors \
    --vae HunyuanVideo/vae/diffusion_pytorch_model.safetensors \
    --text_encoder1 HunyuanVideo_repackaged/split_files/text_encoders/llava_llama3_fp16.safetensors \
    --text_encoder2 HunyuanVideo_repackaged/split_files/text_encoders/clip_l.safetensors \
    --image_encoder sigclip_vision_384/sigclip_vision_patch14_384.safetensors \
    --dataset_config video_config.toml \
    --sdpa --mixed_precision bf16 \
    --optimizer_type adamw8bit --learning_rate 2e-4 --gradient_checkpointing \
    --timestep_sampling shift --weighting_scheme none --discrete_flow_shift 3.0 \
    --max_data_loader_n_workers 2 --persistent_data_loader_workers \
    --network_module networks.lora_framepack --network_dim 32 \
    --max_train_epochs 500 --save_every_n_epochs 1 --seed 42 \
    --output_dir framepack_yichen_output --output_name framepack-yichen-lora
```

```bash
python fpack_generate_video.py \
    --dit FramePackI2V_HY/diffusion_pytorch_model-00001-of-00003.safetensors \
    --vae HunyuanVideo/vae/diffusion_pytorch_model.safetensors \
    --text_encoder1 HunyuanVideo_repackaged/split_files/text_encoders/llava_llama3_fp16.safetensors \
    --text_encoder2 HunyuanVideo_repackaged/split_files/text_encoders/clip_l.safetensors \
    --image_encoder sigclip_vision_384/sigclip_vision_patch14_384.safetensors \
    --image_path fln.png \
    --prompt "In the style of Yi Chen Dancing White Background , The character's movements shift dynamically throughout the video, transitioning from poised stillness to lively dance steps. Her expressions evolve seamlesslyâ€”starting with focused determination, then flashing surprise as she executes a quick spin, before breaking into a joyful smile mid-leap. Her hands flow through choreographed positions, sometimes extending gracefully like unfolding wings, other times clapping rhythmically against her wrists. During a dramatic hip sway, her fingers fan open near her cheek, then sweep downward as her whole body dips into a playful crouch, the sequins on her costume catching the light with every motion." \
    --video_size 960 544 --video_seconds 3 --fps 30 --infer_steps 25 \
    --attn_mode sdpa --fp8_scaled \
    --vae_chunk_size 32 --vae_spatial_tile_sample_min_size 128 \
    --save_path save --output_type both \
    --seed 1234 --lora_multiplier 1.0 --lora_weight framepack_yichen_output/framepack-yichen-lora-000002.safetensors
```




https://github.com/user-attachments/assets/e695d2a7-4145-4cf2-b9b5-2f4ca764ec02


#### å°¼éœ²ä¾‹å­
<br/>

```bash
wget https://huggingface.co/tori29umai/FramePack_LoRA/resolve/main/rotate_landscape_V4.safetensors

huggingface-cli download --repo-type dataset \
    --resume-download \
    svjack/Genshin-Impact-Portrait-with-Tags-Filtered-IID-Gender-SP \
    --include "genshin_impact_NILOU_images_and_texts/*" \
    --local-dir .
```

```python
import os
import glob
import shutil
from tqdm import tqdm
import subprocess
import time

def get_latest_mp4(save_dir):
    """è·å–saveç›®å½•ä¸‹æœ€æ–°åˆ›å»ºçš„mp4æ–‡ä»¶"""
    mp4_files = glob.glob(os.path.join(save_dir, '*.mp4'))
    if not mp4_files:
        return None
    return max(mp4_files, key=os.path.getctime)

def process_images_and_texts():
    # è¾“å…¥å’Œè¾“å‡ºç›®å½•
    input_dir = "genshin_impact_NILOU_images_and_texts"
    output_dir = "genshin_impact_NILOU_FramePack_Rotate_Dancing_Captioned"
    save_dir = "save"

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(save_dir, exist_ok=True)

    # è·å–æ‰€æœ‰.pngæ–‡ä»¶
    png_files = glob.glob(os.path.join(input_dir, '*.png'))

    # å›ºå®šprompt
    prompt = """In the style of Yi Chen Dancing White Background , The character's movements shift dynamically throughout the video, transitioning from poised stillness to lively dance steps. Her expressions evolve seamlesslyâ€”starting with focused determination, then flashing surprise as she executes a quick spin, before breaking into a joyful smile mid-leap. Her hands flow through choreographed positions, sometimes extending gracefully like unfolding wings, other times clapping rhythmically against her wrists. During a dramatic hip sway, her fingers fan open near her cheek, then sweep downward as her whole body dips into a playful crouch, the sequins on her costume catching the light with every motion."""

    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
    for png_file in tqdm(png_files, desc="Processing images"):
        # è·å–å¯¹åº”çš„txtæ–‡ä»¶
        base_name = os.path.splitext(os.path.basename(png_file))[0]
        txt_file = os.path.join(input_dir, f"{base_name}.txt")

        if not os.path.exists(txt_file):
            print(f"Warning: No corresponding .txt file found for {png_file}")
            continue

        # æ„å»ºå‘½ä»¤
        cmd = [
            "python", "fpack_generate_video.py",
            "--dit", "FramePackI2V_HY/diffusion_pytorch_model-00001-of-00003.safetensors",
            "--vae", "HunyuanVideo/vae/diffusion_pytorch_model.safetensors",
            "--text_encoder1", "HunyuanVideo_repackaged/split_files/text_encoders/llava_llama3_fp16.safetensors",
            "--text_encoder2", "HunyuanVideo_repackaged/split_files/text_encoders/clip_l.safetensors",
            "--image_encoder", "sigclip_vision_384/sigclip_vision_patch14_384.safetensors",
            "--image_path", png_file,
            "--prompt", prompt,
            "--video_size", "960", "544",
            "--video_seconds", "3",
            "--fps", "30",
            "--infer_steps", "25",
            "--attn_mode", "sdpa",
            "--fp8_scaled",
            "--vae_chunk_size", "32",
            "--vae_spatial_tile_sample_min_size", "128",
            "--save_path", save_dir,
            "--output_type", "both",
            "--seed", "1234",
            "--lora_multiplier", "1.0",
            "--lora_weight", "rotate_landscape_V4.safetensors"
        ]

        # è¿è¡Œå‘½ä»¤
        try:
            subprocess.run(cmd, check=True)
        except subprocess.CalledProcessError as e:
            print(f"Error processing {png_file}: {e}")
            continue

        # è·å–æœ€æ–°ç”Ÿæˆçš„mp4æ–‡ä»¶
        latest_mp4 = get_latest_mp4(save_dir)
        if latest_mp4 is None:
            print(f"Warning: No .mp4 file generated for {png_file}")
            continue

        # æ„å»ºè¾“å‡ºæ–‡ä»¶å
        output_mp4 = os.path.join(output_dir, f"{base_name}.mp4")
        output_txt = os.path.join(output_dir, f"{base_name}.txt")

        # æ‹·è´æ–‡ä»¶
        shutil.move(latest_mp4, output_mp4)
        shutil.copy2(txt_file, output_txt)

        '''
        # æ¸…ç†saveç›®å½•
        for f in glob.glob(os.path.join(save_dir, '*')):
            try:
                os.remove(f)
            except:
                pass
        '''

if __name__ == "__main__":
    process_images_and_texts()
    print("All processing completed!")
```

#### åŸç¥ç”Ÿè´ºä¾‹å­

```python
#### git clone https://huggingface.co/datasets/svjack/Genshin_Impact_Birthday_Art_Images

import os
import glob
import shutil
from tqdm import tqdm
import subprocess
import time

def get_latest_mp4(save_dir):
    """è·å–saveç›®å½•ä¸‹æœ€æ–°åˆ›å»ºçš„mp4æ–‡ä»¶"""
    mp4_files = glob.glob(os.path.join(save_dir, '*.mp4'))
    if not mp4_files:
        return None
    return max(mp4_files, key=os.path.getctime)

def process_images_and_texts():
    # è¾“å…¥å’Œè¾“å‡ºç›®å½•
    input_dir = "Genshin_Impact_Birthday_Art_Images"
    output_dir = "Genshin_Impact_Birthday_Art_FramePack_Rotate_Dancing_Captioned_New"
    save_dir = "save"

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(save_dir, exist_ok=True)

    # è·å–æ‰€æœ‰.pngæ–‡ä»¶
    png_files_0 = glob.glob(os.path.join(input_dir, '*.png'))
    png_files_1 = glob.glob(os.path.join(input_dir, '*.jpg'))
    png_files_2 = glob.glob(os.path.join(input_dir, '*.jpeg'))
    png_files_3 = glob.glob(os.path.join(input_dir, '*.webp'))
    png_files = list(png_files_0) + list(png_files_1) + list(png_files_2) + list(png_files_3)


    # å›ºå®šprompt
    prompt = """The camera smoothly orbits around the center of the scene, keeping the center point fixed and always in view."""

    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
    for png_file in tqdm(png_files, desc="Processing images"):
        # è·å–å¯¹åº”çš„txtæ–‡ä»¶
        base_name = os.path.splitext(os.path.basename(png_file))[0]
        txt_file = os.path.join(input_dir, f"{base_name}.txt")

        '''
        if not os.path.exists(txt_file):
            print(f"Warning: No corresponding .txt file found for {png_file}")
            continue
        '''
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            "python", "fpack_generate_video.py",
            "--dit", "FramePackI2V_HY/diffusion_pytorch_model-00001-of-00003.safetensors",
            "--vae", "HunyuanVideo/vae/diffusion_pytorch_model.safetensors",
            "--text_encoder1", "HunyuanVideo_repackaged/split_files/text_encoders/llava_llama3_fp16.safetensors",
            "--text_encoder2", "HunyuanVideo_repackaged/split_files/text_encoders/clip_l.safetensors",
            "--image_encoder", "sigclip_vision_384/sigclip_vision_patch14_384.safetensors",
            "--image_path", png_file,
            "--prompt", prompt,
            "--video_size", "768", "768",
            "--video_seconds", "3",
            "--fps", "30",
            "--infer_steps", "25",
            "--attn_mode", "sdpa",
            "--fp8_scaled",
            "--vae_chunk_size", "32",
            "--vae_spatial_tile_sample_min_size", "128",
            "--save_path", save_dir,
            "--output_type", "both",
            "--seed", "1234",
            "--lora_multiplier", "1.0",
            "--lora_weight", "rotate_landscape_V4.safetensors"
        ]

        # è¿è¡Œå‘½ä»¤
        try:
            subprocess.run(cmd, check=True)
        except subprocess.CalledProcessError as e:
            print(f"Error processing {png_file}: {e}")
            continue

        # è·å–æœ€æ–°ç”Ÿæˆçš„mp4æ–‡ä»¶
        latest_mp4 = get_latest_mp4(save_dir)
        if latest_mp4 is None:
            print(f"Warning: No .mp4 file generated for {png_file}")
            continue

        # æ„å»ºè¾“å‡ºæ–‡ä»¶å
        output_mp4 = os.path.join(output_dir, f"{base_name}.mp4")
        output_txt = os.path.join(output_dir, f"{base_name}.txt")

        # æ‹·è´æ–‡ä»¶
        shutil.move(latest_mp4, output_mp4)
        #shutil.copy2(txt_file, output_txt)

        '''
        # æ¸…ç†saveç›®å½•
        for f in glob.glob(os.path.join(save_dir, '*')):
            try:
                os.remove(f)
            except:
                pass
        '''

if __name__ == "__main__":
    process_images_and_texts()
    print("All processing completed!")


```

#### åŸç¥ç”Ÿè´º æ¯”èµ·XXXæˆ‘æ›´å–œæ¬¢ä½  äºŒåˆ›

##### éŸ³é¢‘srtåˆ†å‰²
```python
# git clone https://huggingface.co/datasets/svjack/I_prefer_you_over_something_TWO_VIDEOS


import os
import re
from pydub import AudioSegment
from moviepy.editor import VideoFileClip
from datetime import datetime, timedelta

def parse_srt(srt_content):
    """Parse SRT content into a list of subtitle segments with duration validation"""
    segments = []
    pattern = re.compile(r'(\d+)\n(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})\n(.+?)(?:\n\n|\n$)', re.DOTALL)

    for match in pattern.finditer(srt_content):
        index = int(match.group(1))
        start_time = parse_srt_time(match.group(2))
        end_time = parse_srt_time(match.group(3))
        text = match.group(4).strip()

        # Validate time range
        if end_time <= start_time:
            print(f"Warning: Skipping segment {index} - End time {end_time} <= Start time {start_time}")
            continue

        segments.append((index, start_time, end_time, text))

    return segments

def parse_srt_time(time_str):
    """Convert SRT time format to seconds with validation"""
    try:
        hours, mins, secs = time_str.split(':')
        secs, millis = secs.split(',')
        return timedelta(
            hours=int(hours),
            minutes=int(mins),
            seconds=int(secs),
            milliseconds=int(millis)
        ).total_seconds()
    except Exception as e:
        raise ValueError(f"Invalid time format '{time_str}': {str(e)}")

def sanitize_filename(filename):
    """Sanitize filenames with special characters"""
    # Keep Chinese/Japanese characters and basic symbols
    return re.sub(r'[<>:"/\\|?*\x00-\x1f]', '', filename).strip()

def get_video_duration(video_path):
    """Get accurate video duration in seconds"""
    try:
        with VideoFileClip(video_path) as video:
            return video.duration
    except Exception as e:
        print(f"Warning: Could not get duration for {video_path}: {str(e)}")
        return None

def adjust_segment_times(segments, max_duration):
    """Adjust segment times to fit within video duration"""
    adjusted_segments = []
    for idx, start, end, text in segments:
        # Cap end time at video duration
        if max_duration and end > max_duration:
            print(f"Adjusting segment {idx} end time from {end}s to {max_duration}s")
            end = max_duration

        # Skip if start time is beyond video duration
        if max_duration and start >= max_duration:
            print(f"Skipping segment {idx} - Start time {start}s >= video duration {max_duration}s")
            continue

        # Skip if duration becomes zero after adjustment
        if end <= start:
            print(f"Skipping segment {idx} - Invalid duration after adjustment (start: {start}s, end: {end}s)")
            continue

        adjusted_segments.append((idx, start, end, text))
    return adjusted_segments

def split_media(input_path, srt_content, output_path, output_type='wav'):
    """Improved media splitting with duration validation"""
    os.makedirs(output_path, exist_ok=True)

    try:
        # Get video duration first
        video_duration = None
        if output_type == 'mp4':
            video_duration = get_video_duration(input_path)

        # Parse and validate segments
        segments = parse_srt(srt_content)
        if video_duration:
            segments = adjust_segment_times(segments, video_duration)

        if not segments:
            print("No valid segments to process")
            return False

        input_name = os.path.splitext(os.path.basename(input_path))[0]
        clean_name = sanitize_filename(input_name)

        if output_type == 'wav':
            audio = AudioSegment.from_file(input_path)
            audio_duration = len(audio) / 1000  # pydub uses milliseconds

            for idx, start_time, end_time, text in segments:
                # Convert to milliseconds and validate against audio duration
                start_ms = int(start_time * 1000)
                end_ms = int(end_time * 1000)

                if end_ms > len(audio):
                    end_ms = len(audio)
                    print(f"Adjusting audio segment {idx} end time to {end_ms/1000}s")

                segment = audio[start_ms:end_ms]

                base_name = f"{idx:04d}_{clean_name}"
                audio_file = os.path.join(output_path, f"{base_name}.wav")
                text_file = os.path.join(output_path, f"{base_name}.txt")

                segment.export(audio_file, format='wav')
                with open(text_file, 'w', encoding='utf-8') as f:
                    f.write(text)

        elif output_type == 'mp4':
            with VideoFileClip(input_path) as video:
                for idx, start_time, end_time, text in segments:
                    # Ensure times are within video duration
                    if end_time > video.duration:
                        end_time = video.duration
                        print(f"Adjusting video segment {idx} end time to {end_time}s")

                    base_name = f"{idx:04d}_{clean_name}"
                    video_file = os.path.join(output_path, f"{base_name}.mp4")
                    text_file = os.path.join(output_path, f"{base_name}.txt")

                    segment = video.subclip(start_time, end_time)
                    segment.write_videofile(
                        video_file,
                        codec='libx264',
                        audio_codec='aac',
                        verbose=False,
                        threads=4,
                        preset='fast',
                        ffmpeg_params=['-movflags', '+faststart']
                    )
                    with open(text_file, 'w', encoding='utf-8') as f:
                        f.write(text)

        return True

    except Exception as e:
        print(f"Error processing {input_path}: {str(e)}")
        return False

# Main execution with enhanced error handling
if __name__ == "__main__":
    video_files = [
        "trimmed_ğŸ™‹æ¯”èµ·ğŸ¦æˆ‘æ›´å–œæ¬¢å›½ç”·v.mp4",
        "trimmed_[åŸç¥åŠ¨ç”»]æˆ‘æ›´å–œæ¬¢ä½ ï¼æ¯”å¿ƒ.mp4"
    ]

    # SRT contents
    srt_contents = {
    "ja": """
1
00:00:00,000 --> 00:00:02,050
ã©ã†ã

2
00:00:02,050 --> 00:00:04,050
ä½•ãŒå¥½ãï¼Ÿ

3
00:00:04,050 --> 00:00:07,100
ãƒŸãƒ³ãƒˆã‚ˆã‚Šã‚‚ã‚ãªãŸãŒå¥½ã

4
00:00:07,100 --> 00:00:11,100
ã‚«ã‚¿ãƒˆã¡ã‚ƒã‚“ã€ä½•ãŒå¥½ãï¼Ÿ

5
00:00:11,100 --> 00:00:14,300
ã‚¹ãƒˆãƒ­ãƒ™ãƒªãƒ¼ã‚ˆã‚Šã‚‚ã‚ãªãŸãŒå¥½ã

6
00:00:14,300 --> 00:00:18,150
èµ¤ã¡ã‚ƒã‚“ã€ä½•ãŒå¥½ãï¼Ÿ

7
00:00:18,150 --> 00:00:23,000
ã‚¯ãƒƒã‚­ãƒ¼ã‚¯ãƒªãƒ¼ãƒ ã‚ˆã‚Šã‚‚ã‚ãªãŸãŒå¥½ã
""",
    "zh": """
1
00:00:00,000 --> 00:00:02,050
è¯·è¯´å§

2
00:00:02,050 --> 00:00:04,050
ä½ å–œæ¬¢ä»€ä¹ˆï¼Ÿ

3
00:00:04,050 --> 00:00:07,100
æ¯”èµ·è–„è·å‘³æˆ‘æ›´å–œæ¬¢ä½ 

4
00:00:07,100 --> 00:00:11,100
å¡å¡”æ‰˜é…±ï¼Œä½ å–œæ¬¢ä»€ä¹ˆï¼Ÿ

5
00:00:11,100 --> 00:00:14,300
æ¯”èµ·è‰è“å‘³æˆ‘æ›´å–œæ¬¢ä½ 

6
00:00:14,300 --> 00:00:18,150
å®å®å–œæ¬¢ä»€ä¹ˆï¼Ÿ

7
00:00:18,150 --> 00:00:23,000
æ¯”èµ·é¥¼å¹²å¥¶æ²¹å‘³æˆ‘æ›´å–œæ¬¢ä½ 
""",
    "en": """
1
00:00:00,000 --> 00:00:02,050
Please tell me

2
00:00:02,050 --> 00:00:04,050
What do you like?

3
00:00:04,050 --> 00:00:07,100
I love you more than mint

4
00:00:07,100 --> 00:00:11,100
Katato-chan, what do you like?

5
00:00:11,100 --> 00:00:14,300
I love you more than strawberry

6
00:00:14,300 --> 00:00:18,150
Baby, what do you like?

7
00:00:18,150 --> 00:00:23,000
I love you more than cookies and cream
"""
}
    #output_types = ['mp4', 'wav']
    output_types = ['wav']

    for video in video_files:
        if not os.path.exists(video):
            print(f"Error: Input file not found - {video}")
            continue

        for lang, srt in srt_contents.items():
            for out_type in output_types:
                base_name = os.path.splitext(video)[0]
                output_dir = f"output_{lang}_{out_type}_{sanitize_filename(base_name)}"

                print(f"\nProcessing: {video} | {lang} | {out_type}")
                print(f"Output to: {output_dir}")

                success = split_media(
                    input_path=video,
                    srt_content=srt,
                    output_path=output_dir,
                    output_type=out_type
                )

                if success:
                    print("âœ“ Successfully processed")
                else:
                    print("âœ— Processing failed")

```

##### åˆçº§éŸ³è§†é¢‘æ‹¼æ¥
```python
#git clone https://huggingface.co/datasets/svjack/I_prefer_you_over_something_GIRL_AUDIOS_SPLITED
#git clone https://huggingface.co/datasets/svjack/I_prefer_you_over_something_BOY_AUDIOS_SPLITED
#git clone https://huggingface.co/datasets/svjack/Genshin-Impact-Portrait-with-Tags-Filtered-IID-Gender-SP
#git clone https://huggingface.co/datasets/svjack/Genshin_Impact_Birthday_Art_FramePack_Rotate_Named


import os
import random
import pandas as pd
from itertools import combinations
import subprocess
from moviepy.editor import concatenate_videoclips, AudioFileClip, ImageClip, CompositeAudioClip, VideoFileClip
from moviepy.video.fx import all as vfx
from moviepy.video.fx.all import crop
from moviepy.audio.AudioClip import AudioClip
from PIL import Image
import numpy as np

# é…ç½®å‚æ•°
config = {
    'num_images_per_char': 3,  # æ¯ä¸ªè§’è‰²ä½¿ç”¨çš„å›¾ç‰‡æ•°é‡
    'output_resolution': (1024, 1024),  # è¾“å‡ºè§†é¢‘åˆ†è¾¨ç‡
    'image_duration': 3,  # æ¯å¼ å›¾ç‰‡æ˜¾ç¤ºæ—¶é•¿(ç§’)
    'video_duration': 5,  # æ¯ä¸ªè§†é¢‘ç‰‡æ®µæ—¶é•¿(ç§’)
    'audio_fade_duration': 0.5,  # éŸ³é¢‘æ·¡å…¥æ·¡å‡ºæ—¶é•¿(ç§’)
}

# è§’è‰²åå­—æ˜ å°„å’Œæ€§åˆ«æ˜ å°„
name_mapping = {
    'èŠ­èŠ­æ‹‰': 'BARBARA', 'æŸ¯è±': 'COLLEI', 'é›·ç”µå°†å†›': 'RAIDEN SHOGUN', 'äº‘å ‡': 'YUN JIN',
    'å…«é‡ç¥å­': 'YAE MIKO', 'å¦®éœ²': 'NILOU', 'ç»®è‰¯è‰¯': 'KIRARA', 'ç ‚ç³–': 'SUCROSE',
    'çéœ²çŠ': 'FARUZAN', 'ç³å¦®ç‰¹': 'LYNETTE', 'çº³è¥¿å¦²': 'NAHIDA', 'è¯ºè‰¾å°”': 'NOELLE',
    'å‡å…‰': 'NINGGUANG', 'é¹¿é‡é™¢å¹³è—': 'HEIZOU', 'ç´': 'JEAN', 'æ«åŸä¸‡å¶': 'KAEDEHARA KAZUHA',
    'èŠ™å®å¨œ': 'FURINA', 'è‰¾å°”æµ·æ£®': 'ALHAITHAM', 'ç”˜é›¨': 'GANYU', 'å‡¯äºš': 'KAEYA',
    'è’æ³·ä¸€æ–—': 'ARATAKI ITTO', 'ä¼˜èˆ': 'EULA', 'è¿ªå¥¥å¨œ': 'DIONA', 'æ¸©è¿ª': 'VENTI',
    'ç¥é‡Œç»«äºº': 'KAMISATO AYATO', 'é˜¿è´å¤š': 'ALBEDO', 'é‡äº‘': 'CHONGYUN', 'é’Ÿç¦»': 'ZHONGLI',
    'è¡Œç§‹': 'XINGQIU', 'èƒ¡æ¡ƒ': 'HU TAO', 'é­ˆ': 'XIAO', 'èµ›è¯º': 'CYNO',
    'ç¥é‡Œç»«å': 'KAMISATO AYAKA', 'äº”éƒ': 'GOROU', 'æ—å°¼': 'LYNEY', 'è¿ªå¢å…‹': 'DILUC',
    'å®‰æŸ': 'AMBER', 'çƒŸç»¯': 'YANFEI', 'å®µå®«': 'YOIMIYA', 'çŠç‘šå®«å¿ƒæµ·': 'SANGONOMIYA KOKOMI',
    'ç½—èè‰äºš': 'ROSARIA', 'ä¸ƒä¸ƒ': 'QIQI', 'ä¹…å²å¿': 'KUKI SHINOBU', 'ç”³é¹¤': 'SHENHE',
    'æ‰˜é©¬': 'THOMA', 'èŠ™å¯§å¨œ': 'FURINA', 'é›·æ³½': 'RAZOR'
}

gender_mapping = {
    'ä¹…å²å¿': 'å¥³', 'äº‘å ‡': 'å¥³', 'äº”éƒ': 'ç”·', 'ä¼˜èˆ': 'å¥³', 'å‡å…‰': 'å¥³', 'å‡¯äºš': 'ç”·',
    'å®‰æŸ': 'å¥³', 'å®µå®«': 'å¥³', 'æ¸©è¿ª': 'ç”·', 'çƒŸç»¯': 'å¥³', 'çŠç‘šå®«å¿ƒæµ·': 'å¥³', 'ç´': 'å¥³',
    'ç”˜é›¨': 'å¥³', 'ç”³é¹¤': 'å¥³', 'ç ‚ç³–': 'å¥³', 'ç¥é‡Œç»«äºº': 'ç”·', 'ç¥é‡Œç»«å': 'å¥³', 'ç»®è‰¯è‰¯': 'å¥³',
    'ç½—èè‰äºš': 'å¥³', 'èƒ¡æ¡ƒ': 'å¥³', 'è‰¾å°”æµ·æ£®': 'ç”·', 'è’æ³·ä¸€æ–—': 'ç”·', 'è¡Œç§‹': 'ç”·', 'è¯ºè‰¾å°”': 'å¥³',
    'è¿ªå¢å…‹': 'ç”·', 'è¿ªå¥¥å¨œ': 'å¥³', 'é‡äº‘': 'ç”·', 'é’Ÿç¦»': 'ç”·', 'é˜¿è´å¤š': 'ç”·', 'é›·æ³½': 'ç”·',
    'é›·ç”µå°†å†›': 'å¥³', 'é­ˆ': 'ç”·', 'é¹¿é‡é™¢å¹³è—': 'ç”·'
}

# è·¯å¾„é…ç½®
paths = {
    'female_audios': 'I_prefer_you_over_something_GIRL_AUDIOS_SPLITED',
    'male_audios': 'I_prefer_you_over_something_BOY_AUDIOS_SPLITED',
    'images': 'Genshin-Impact-Portrait-with-Tags-Filtered-IID-Gender-SP',
    'videos': 'Genshin_Impact_Birthday_Art_FramePack_Rotate_Named'
}

# åŠ è½½è§†é¢‘å…ƒæ•°æ®
video_metadata = pd.read_csv(os.path.join(paths['videos'], 'metadata.csv'))

def get_character_resources(gender, num_groups):
    """è·å–æŒ‡å®šæ€§åˆ«çš„è§’è‰²ç»„åˆå’Œèµ„æº"""
    # æŒ‰æ€§åˆ«ç­›é€‰è§’è‰²
    chars = [name for name, g in gender_mapping.items() if g == gender]

    # ç”Ÿæˆæ‰€æœ‰3è§’è‰²ç»„åˆ
    all_combinations = list(combinations(chars, 3))
    random.shuffle(all_combinations)

    # åªå–éœ€è¦çš„æ•°é‡
    selected_combinations = all_combinations[:num_groups]

    results = []
    for combo in selected_combinations:
        group_data = []
        for char in combo:
            # è·å–è§’è‰²è‹±æ–‡å
            en_name = name_mapping.get(char, char).replace(' ', '_').upper()

            # è·å–éŸ³é¢‘æ–‡ä»¶
            audio_dir = paths['female_audios'] if gender == 'å¥³' else paths['male_audios']
            audio_files = sorted([f for f in os.listdir(audio_dir) if f.endswith('.wav') or f.endswith('.mp3')])

            # è·å–å›¾ç‰‡æ–‡ä»¶
            image_dir = os.path.join(paths['images'], f'genshin_impact_{en_name}_images_and_texts')
            image_files = []
            if os.path.exists(image_dir):
                image_files = [f for f in os.listdir(image_dir) if f.lower().endswith('.png')]
                random.shuffle(image_files)
                image_files = image_files[:config['num_images_per_char']]

            # è·å–è§†é¢‘æ–‡ä»¶
            video_files = video_metadata[video_metadata['prompt'] == char]['file_name'].tolist()

            group_data.append({
                'name': char,
                'en_name': en_name,
                'audio_files': audio_files,
                'image_files': [os.path.join(image_dir, f) for f in image_files],
                'video_files': [os.path.join(paths['videos'], f) for f in video_files]
            })

        results.append(group_data)

    return results

def create_video_clip(group_data, output_path):
    """ä¸ºå•ä¸ªè§’è‰²ç»„åˆåˆ›å»ºè§†é¢‘ï¼ˆæ ¹æ®éŸ³é¢‘æ—¶é—´åˆ†é…å›¾ç‰‡æ˜¾ç¤ºæ—¶é—´ï¼‰"""
    clips = []
    audio_clips = []
    current_time = 0  # è·Ÿè¸ªå½“å‰æ—¶é—´ä½ç½®

    # è·å–éŸ³é¢‘æ–‡ä»¶
    audio_dir = paths['female_audios'] if gender_mapping[group_data[0]['name']] == 'å¥³' else paths['male_audios']
    audio_files = sorted([f for f in os.listdir(audio_dir) if f.endswith(('.wav', '.mp3'))])

    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„éŸ³é¢‘æ–‡ä»¶ï¼ˆ7ä¸ªï¼‰
    if len(audio_files) < 7:
        raise ValueError("éŸ³é¢‘æ–‡ä»¶ä¸è¶³7ä¸ªï¼Œæ— æ³•ç”Ÿæˆè§†é¢‘")

    # åŠ è½½æ‰€æœ‰éŸ³é¢‘å¹¶è®°å½•æ—¶é•¿
    audio_durations = []
    for i in range(7):  # åŠ è½½7ä¸ªéŸ³é¢‘
        audio_path = os.path.join(audio_dir, audio_files[i])
        audio_clip = AudioFileClip(audio_path)
        audio_durations.append(audio_clip.duration)
        audio_clips.append(audio_clip)

    def apply_effects(img_clip):
        """å¯¹å›¾ç‰‡å‰ªè¾‘åº”ç”¨ç‰¹æ•ˆ"""
        # è½»å¾®æ—‹è½¬å’Œç¼©æ”¾
        img_clip = img_clip.fx(vfx.rotate, angle=lambda t: 5 * np.sin(2 * np.pi * t / img_clip.duration), expand=False)
        img_clip = img_clip.fx(vfx.resize, lambda t: 1 + 0.1 * np.sin(2 * np.pi * t / img_clip.duration))

        # æ·¡å…¥æ·¡å‡ºæ•ˆæœ
        img_clip = img_clip.fx(vfx.fadein, 0.1).fx(vfx.fadeout, 0.1)

        return img_clip

    def apply_video_effects(video_clip):
        """å¯¹è§†é¢‘å‰ªè¾‘åº”ç”¨ç‰¹æ•ˆï¼Œåˆ†ä¸‰æ¬¡åº”ç”¨"""
        duration = video_clip.duration
        third_duration = duration / 3

        def apply_single_effect(clip):
            # è½»å¾®æ—‹è½¬å’Œç¼©æ”¾
            clip = clip.fx(vfx.rotate, angle=lambda t: 5 * np.sin(2 * np.pi * t / clip.duration), expand=False)
            clip = clip.fx(vfx.resize, lambda t: 1 + 0.1 * np.sin(2 * np.pi * t / clip.duration))
            return clip

        # åˆ†å‰²è§†é¢‘å‰ªè¾‘ä¸ºä¸‰ä¸ªéƒ¨åˆ†
        part1 = video_clip.subclip(0, third_duration)
        part2 = video_clip.subclip(third_duration, 2 * third_duration)
        part3 = video_clip.subclip(2 * third_duration, duration)

        # å¯¹æ¯ä¸ªéƒ¨åˆ†åº”ç”¨ç‰¹æ•ˆ
        part1 = apply_single_effect(part1)
        part2 = apply_single_effect(part2)
        part3 = apply_single_effect(part3)

        # åˆå¹¶ä¸‰ä¸ªéƒ¨åˆ†
        final_video_clip = concatenate_videoclips([part1, part2, part3])
        return final_video_clip

    # è§’è‰²1: 3å›¾ç‰‡å…±äº«éŸ³é¢‘1+2æ—¶é•¿ï¼Œè§†é¢‘ä½¿ç”¨éŸ³é¢‘3
    char1 = group_data[0]
    char1_audio_duration = audio_durations[0] + audio_durations[1]

    # è®¡ç®—æ¯å¼ å›¾ç‰‡çš„æ˜¾ç¤ºæ—¶é—´ï¼ˆå¹³å‡åˆ†é…ï¼‰
    if len(char1['image_files']) >= 3:
        per_image_duration = char1_audio_duration / 3
        for i in range(3):
            img_clip = ImageClip(char1['image_files'][i], duration=per_image_duration)
            img_clip = img_clip.resize(config['output_resolution'])
            img_clip = apply_effects(img_clip)
            clips.append(img_clip)
            current_time += per_image_duration

    # è§†é¢‘1ä½¿ç”¨éŸ³é¢‘3
    if len(char1['video_files']) > 0:
        video_clip = VideoFileClip(char1['video_files'][0])
        original_duration = video_clip.duration
        target_duration = audio_durations[2]
        speed_factor = original_duration / target_duration
        video_clip = video_clip.fx(vfx.speedx, speed_factor)
        video_clip = video_clip.resize(config['output_resolution'])
        video_clip = apply_video_effects(video_clip)
        clips.append(video_clip)
        current_time += target_duration

    # è§’è‰²2: 3å›¾ç‰‡å…±äº«éŸ³é¢‘4æ—¶é•¿ï¼Œè§†é¢‘ä½¿ç”¨éŸ³é¢‘5
    char2 = group_data[1]
    char2_audio_duration = audio_durations[3]

    if len(char2['image_files']) >= 3:
        per_image_duration = char2_audio_duration / 3
        for i in range(3):
            img_clip = ImageClip(char2['image_files'][i], duration=per_image_duration)
            img_clip = img_clip.resize(config['output_resolution'])
            img_clip = apply_effects(img_clip)
            clips.append(img_clip)
            current_time += per_image_duration

    # è§†é¢‘2ä½¿ç”¨éŸ³é¢‘5
    if len(char2['video_files']) > 0:
        video_clip = VideoFileClip(char2['video_files'][0])
        original_duration = video_clip.duration
        target_duration = audio_durations[4]
        speed_factor = original_duration / target_duration
        video_clip = video_clip.fx(vfx.speedx, speed_factor)
        video_clip = video_clip.resize(config['output_resolution'])
        video_clip = apply_video_effects(video_clip)
        clips.append(video_clip)
        current_time += target_duration

    # è§’è‰²3: 3å›¾ç‰‡å…±äº«éŸ³é¢‘6æ—¶é•¿ï¼Œè§†é¢‘ä½¿ç”¨éŸ³é¢‘7
    char3 = group_data[2]
    char3_audio_duration = audio_durations[5]

    if len(char3['image_files']) >= 3:
        per_image_duration = char3_audio_duration / 3
        for i in range(3):
            img_clip = ImageClip(char3['image_files'][i], duration=per_image_duration)
            img_clip = img_clip.resize(config['output_resolution'])
            img_clip = apply_effects(img_clip)
            clips.append(img_clip)
            current_time += per_image_duration

    # è§†é¢‘3ä½¿ç”¨éŸ³é¢‘7
    if len(char3['video_files']) > 0:
        video_clip = VideoFileClip(char3['video_files'][0])
        original_duration = video_clip.duration
        target_duration = audio_durations[6]
        speed_factor = original_duration / target_duration
        video_clip = video_clip.fx(vfx.speedx, speed_factor)
        video_clip = video_clip.resize(config['output_resolution'])
        video_clip = apply_video_effects(video_clip)
        clips.append(video_clip)
        current_time += target_duration

    # åˆå¹¶è§†é¢‘ç‰‡æ®µ
    final_video = concatenate_videoclips(clips, method="compose")

    # åˆå¹¶éŸ³é¢‘ï¼ˆä¸¥æ ¼å¯¹é½ï¼‰
    aligned_audio_clips = []
    current_audio_time = 0

    # è§’è‰²1éŸ³é¢‘ï¼ˆéŸ³é¢‘1+2ç”¨äºå›¾ç‰‡ï¼ŒéŸ³é¢‘3ç”¨äºè§†é¢‘ï¼‰
    aligned_audio_clips.append(audio_clips[0].set_start(current_audio_time))
    current_audio_time += audio_durations[0]
    aligned_audio_clips.append(audio_clips[1].set_start(current_audio_time))
    current_audio_time += audio_durations[1]
    aligned_audio_clips.append(audio_clips[2].set_start(current_audio_time))
    current_audio_time += audio_durations[2]

    # è§’è‰²2éŸ³é¢‘ï¼ˆéŸ³é¢‘4ç”¨äºå›¾ç‰‡ï¼ŒéŸ³é¢‘5ç”¨äºè§†é¢‘ï¼‰
    aligned_audio_clips.append(audio_clips[3].set_start(current_audio_time))
    current_audio_time += audio_durations[3]
    aligned_audio_clips.append(audio_clips[4].set_start(current_audio_time))
    current_audio_time += audio_durations[4]

    # è§’è‰²3éŸ³é¢‘ï¼ˆéŸ³é¢‘6ç”¨äºå›¾ç‰‡ï¼ŒéŸ³é¢‘7ç”¨äºè§†é¢‘ï¼‰
    aligned_audio_clips.append(audio_clips[5].set_start(current_audio_time))
    current_audio_time += audio_durations[5]
    aligned_audio_clips.append(audio_clips[6].set_start(current_audio_time))

    final_audio = CompositeAudioClip(aligned_audio_clips)
    final_video.audio = final_audio

    # å†™å…¥è¾“å‡ºæ–‡ä»¶
    final_video.write_videofile(output_path, fps=24, codec='libx264', audio_codec='aac')



def generate_videos(gender, num_groups, output_dir='output'):
    """ç”ŸæˆæŒ‡å®šæ•°é‡çš„è§†é¢‘"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # è·å–è§’è‰²ç»„åˆå’Œèµ„æº
    character_groups = get_character_resources(gender, num_groups)

    #print(character_groups)

    # ä¸ºæ¯ä¸ªç»„åˆåˆ›å»ºè§†é¢‘
    for i, group in enumerate(character_groups):
        output_path = os.path.join(output_dir, f'{gender}_group_{i+1}.mp4')
        print(f"æ­£åœ¨ç”Ÿæˆè§†é¢‘: {output_path}")
        print(f"è§’è‰²ç»„åˆ: {[char['name'] for char in group]}")

        try:
            create_video_clip(group, output_path)
            print(f"æˆåŠŸç”Ÿæˆ: {output_path}")
        except Exception as e:
            print(f"ç”Ÿæˆè§†é¢‘å¤±è´¥: {e}")
            continue

generate_videos('å¥³', 2)

generate_videos('ç”·', 2)
```
